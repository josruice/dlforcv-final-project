{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frictionless Product Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Course:** COMS W 4995 - Deep Learning for Computer Vision\n",
    "- **Assignment**: Final project\n",
    "- **Author:** Jose Vicente Ruiz Cepeda (jr3660)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Relendo](https://www.relendo.com) is a Spanish peer to peer rental marketplace the author of this project cofounded in October 2014. The company has been running for more than two and half years now, and it provides its users with a platform where they can safely offer products they don't use everyday, like high-end cameras, tools, sports equipment, etc, to others for them to rent. It provides a different way of consumption, where experience is valued over ownership.\n",
    "\n",
    "<div style=\"clear: both; width: 90%; margin: 20px auto\">\n",
    "    <img style=\"display: inline-block\" width=\"650px\" src=\"images/marketplace.png\">\n",
    "    <img style=\"display: inline-block; margin-left: 20px\" width=\"160px\" src=\"images/categories.png\">\n",
    "</div>\n",
    "\n",
    "As in many other well-known marketplaces, to offer a product the user is required to upload it to the platform. One of the steps of this process is to assign a category to the product and, in some cases, a subcategory. This helps the platform organize the available products, so that other users can find them easily. On the other hand, this means an extra step in the process (usually refered as funnel, because of its shape when it is considered that the number of users that get to each step decreases with the number of steps), an extra _friction_, that the users have to overtake to finally achieve what they want: upload their product to offer it in the platform.\n",
    "\n",
    "As for the business, it is of its own interest to reduce all these frictions to a minimum, since they decrease the number of users that complete successfully the interaction by percentages as high as 11% [1]. Exactly in this step, is where Computer Vision and Deep Learning can help solve the problem. In the case of Relendo, a corpus of images of products uploaded by the users had already been manually categorized by them, so this information can be leveraged to create a classifier that will reduce the friction in the categorization process.\n",
    "\n",
    "In this report, we will go over the details of how a _frictionless_ product categorizer for Relendo was built leveraging its dataset and existing Neural Network architectures pre-trained in the famous ImageNet dataset [2]. \n",
    "Additionally, we will show how this ideas was adapted to also solve the subcategorization problem, where multiple subcategories might be correct at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorization problem, the Deep Learning architectures that follow has been trained and tested in a corpus of 19,393 images related with 11,962 products, divided among 8 categories: \n",
    "0. Photography and video (`photo`) \n",
    "0. Events (`events`)\n",
    "0. Tools (`tools`)\n",
    "0. Electronics (`electronics`)\n",
    "0. Sports (`sports`)\n",
    "0. Musical instruments (`instruments`)\n",
    "0. Caravans and campers (`caravans`)\n",
    "0. Others (`others`)\n",
    "\n",
    "In the cases where a product had multiple images, all of them have been assigned the same category. The distribution of the images is unbalanced, as they come from a real environment and some categories are more popular than others:\n",
    "\n",
    "<img style=\"\" width=\"500px\" src=\"images/products-distribution.png\">\n",
    "\n",
    "This set of images has been splitted in the following way:\n",
    "- 10% test\n",
    "- 18% validation (20% of 90%)\n",
    "- 72% train (80% of 90%)\n",
    "\n",
    "Some examples of the images included in the dataset are the following:\n",
    "\n",
    "<img style=\"\" width=\"900px\" src=\"images/products-grid-2.png\">\n",
    "\n",
    "As can be seen, products are usually centered in the image and there is a variety of backgrounds. Let's see some products in finer detail, including the category they belong to. This images has been rescaled, in the same way they are when inputed to the different networks:\n",
    "\n",
    "<div style=\"margin:20px auto;\">\n",
    "    <img style=\"margin:auto; display:inline-block\" width=\"225px\" src=\"images/products-sample-1.png\">\n",
    "    <img style=\"margin:auto; display:inline-block\" width=\"225px\" src=\"images/products-sample-2.png\">\n",
    "    <img style=\"margin:auto; display:inline-block\" width=\"225px\" src=\"images/products-sample-3.png\">\n",
    "    <img style=\"margin:auto; display:inline-block\" width=\"225px\" src=\"images/products-sample-4.png\">\n",
    "    <img style=\"margin:auto; display:inline-block\" width=\"225px\" src=\"images/products-sample-5.png\">\n",
    "    <img style=\"margin:auto; display:inline-block\" width=\"225px\" src=\"images/products-sample-6.png\">\n",
    "    <img style=\"margin:auto; display:inline-block\" width=\"225px\" src=\"images/products-sample-7.png\">\n",
    "    <img style=\"margin:auto; display:inline-block\" width=\"225px\" src=\"images/products-sample-8.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the implementation part, and taking into account what we have learned in the course, the fine-tuning approach seemed the most reasonable. This consists on using a Neural Network previously trained (pre-trained) in another dataset, to then change the top part of the network (the one that takes care of the final classification step) to adapt it to our current problem.\n",
    "\n",
    "After this modification, the network has to be trained in the new problem set. For that, the new top part is trained first for some epochs, \"freezing\" the rest of the layers, so that their weights don't change. Afterwards, the whole network is trained, \"unfreezing\" all the layers, so that the weights can properly adapt to the changes in the loss.\n",
    "\n",
    "Keras [3], a framework for Deep Learning that runs on top of TensorFlow and Theano, released several months ago some famous architectures along with their weights after training them on ImageNet. The architectures published, all of which have been tested in this project, are the following:\n",
    "\n",
    "- **VGG16** [4]: a 16-layer Convolutional Neural Network used by the Visual Geometry Group (VGG) at Oxford in the ILSVRC-2014 competition.\n",
    "\n",
    "<img style=\"\" width=\"400px\" src=\"images/vgg16.png\">\n",
    "\n",
    "- **VGG19** [4]: an update of the previous network, this time including 19 layers.\n",
    "- **ResNet50** [5]: a 50-layer network architecture proposed by researchers at Microsoft that included for the first time the idea of residuals as a way to increase the training effectiveness of deep networks. An example of one of those modules is the following:\n",
    "\n",
    "<img style=\"\" width=\"130px\" src=\"images/resnet.png\">\n",
    "\n",
    "- **InceptionV3** [6]: an architecture proposed by Google researchers that achieved the highest performance in ILSVRC-2014 competition by including what they call \"inception modules\", that are able to extract features at different scales (1x1, 3x3, 5x5). The first version was named _GoogLeNet_, whereas the used in this project is the third version. An example of one of the inception modules is:\n",
    "\n",
    "<img style=\"\" width=\"400px\" src=\"images/inception.png\">\n",
    "\n",
    "- **Xception** [7]: a modification of the Inception architecture where the \"inception modules\" are replaced by depthwise separable convolutions that was proposed by the creator of Keras.\n",
    "\n",
    "More details about the implementations are available in their respective papers (see the references)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A heavy use is made of Keras models and utilities throught this project. In particular, apart from the architectures explained above, useful callbacks have been leveraged, like `TensorBoard`, that logs the training results in a format that can later be analyzed using the tool with the same name, or `ModelCheckpoint` that can be configured to store the best model after every epoch.\n",
    "\n",
    "As a side note, Keras has been used with TensorFlow as backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras # Keras 1.2.2 assumed.\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications import Xception\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Required to avoid errors with the images.\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    0: 'photo',\n",
    "    1: 'electronics',\n",
    "    2: 'events',\n",
    "    3: 'instruments',\n",
    "    4: 'tools',\n",
    "    5: 'sports',\n",
    "    6: 'caravans',\n",
    "    7: 'others'\n",
    "}\n",
    "\n",
    "SUBCATEGORIES = {\n",
    "    0:  '360',\n",
    "    1:  'action_cameras',\n",
    "    2:  'drones',\n",
    "    3:  'instant_cameras',\n",
    "    4:  'kits',\n",
    "    5:  'lenses',\n",
    "    6:  'lighting',\n",
    "    7:  'others',\n",
    "    8:  'photo_cameras',\n",
    "    9:  'projectors',\n",
    "    10: 'sound',\n",
    "    11: 'supports',\n",
    "    12: 'video_cameras'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modified from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=60)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"%d%%\" % int(cm[i, j]*100),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_sample(X, y, class_names, prediction=None):\n",
    "    plt.imshow(X)\n",
    "    \n",
    "    if prediction != None:\n",
    "        plt.title(\"Class = %s, Predict = %s\" % (class_names[y], class_names[prediction]))\n",
    "    else:\n",
    "        plt.title(\"Class = %s\" % (class_names[y]))\n",
    "\n",
    "    plt.axis('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I - Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories_train_data_dir = './data/train'\n",
    "categories_validation_data_dir = './data/validation'\n",
    "categories_test_data_dir = './data/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Categorizer` class has been defined and implemented to encapsulate all the implementation details in a single structure agnostic of the network architecture used. It has also been developed so that it works both in the categorization and subcategorization problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Categorizer:\n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "        \n",
    "\n",
    "    def build_model(self, \n",
    "                    ImagenetModel=ResNet50,\n",
    "                    weights='imagenet',\n",
    "                    num_dense_layer_units=256,\n",
    "                    dropout=0.5,\n",
    "                    multiclass_output=False,\n",
    "                    verbose=False):\n",
    "        # Keras models are functions, not classes, so we have to check which one is it like this.\n",
    "        if ImagenetModel.func_name == 'InceptionV3' or ImagenetModel.func_name == 'Xception':\n",
    "            self.img_width, self.img_height = 299, 299\n",
    "        else:\n",
    "            self.img_width, self.img_height = 224, 224\n",
    "\n",
    "        # First, let's load the model with ImageNet weights and without the top layer.\n",
    "        # This will take some time the first time, since the weights have to be downloaded.\n",
    "        model_weights = weights if weights == 'imagenet' else None\n",
    "        model = ImagenetModel(\n",
    "            weights=model_weights,\n",
    "            include_top=False,\n",
    "            input_shape=(self.img_width, self.img_height, 3)\n",
    "        )\n",
    "        self.bottom_model = model\n",
    "\n",
    "        # Now, let's create the top layers adapted to our problem. \n",
    "        preds = self._build_top_model(\n",
    "            num_dense_layer_units=num_dense_layer_units,\n",
    "            dropout=dropout,\n",
    "            multiclass_output=multiclass_output)\n",
    "\n",
    "        # Combine both models to get the final one.\n",
    "        self.model = Model(model.input, preds)\n",
    "\n",
    "        if verbose:\n",
    "            self.model.summary()\n",
    "            \n",
    "        # If weights is a string different from ImageNet, path to weights is assumed.\n",
    "        if os.path.exists(weights):\n",
    "            self.model.load_weights(weights)\n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def change_top_model(self,\n",
    "                         new_classes,\n",
    "                         num_dense_layer_units=256,\n",
    "                         dropout=0.5,\n",
    "                         multiclass_output=False,\n",
    "                         verbose=False):\n",
    "        self.classes = new_classes\n",
    "        model = self.bottom_model\n",
    "        preds = self._build_top_model(\n",
    "            num_dense_layer_units=num_dense_layer_units,\n",
    "            dropout=dropout,\n",
    "            multiclass_output=multiclass_output)\n",
    "\n",
    "        self.model = Model(model.input, preds)\n",
    "        if verbose:\n",
    "            self.model.summary()\n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def compile_model(self, optimizer=optimizers.Adagrad(lr=0.001)):\n",
    "        self.is_compiled = True\n",
    "        self.optimizer = optimizer\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "            \n",
    "        return self\n",
    "    \n",
    "\n",
    "    def fine_tune(self,\n",
    "                  train_data_dir,\n",
    "                  validation_data_dir,\n",
    "                  batch_size=16,\n",
    "                  num_only_top_epochs=10,\n",
    "                  num_whole_model_epochs=40,\n",
    "                  best_model_path=None,\n",
    "                  tensorboard_logs_path=None,\n",
    "                  reduce_learning_rate=True):\n",
    "        \n",
    "        # Augmentation configurations for training and validation.\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "        validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            train_data_dir,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=batch_size,\n",
    "            classes=self.classes.values(),\n",
    "            class_mode='categorical')\n",
    "\n",
    "        validation_generator = validation_datagen.flow_from_directory(\n",
    "            validation_data_dir,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=batch_size,\n",
    "            classes=self.classes.values(),\n",
    "            class_mode='categorical')\n",
    "        \n",
    "        if num_only_top_epochs > 0:\n",
    "            # Freeze everything except the top layers, before training.\n",
    "            for layer in self.model.layers[:-self.num_top_layers]:\n",
    "                layer.trainable = False \n",
    "\n",
    "            print \"Starting with top layers training...\"\n",
    "            self._fit_generator(\n",
    "                train_generator,\n",
    "                validation_generator,\n",
    "                batch_size,\n",
    "                num_only_top_epochs,\n",
    "                best_model_path=best_model_path,\n",
    "                tensorboard_logs_path=tensorboard_logs_path,\n",
    "                tensorboard_logs_path_suffix='Top',\n",
    "                reduce_learning_rate=True)\n",
    "            print \"Top layers training done.\"\n",
    "            \n",
    "        if num_whole_model_epochs > 0:\n",
    "            # Unfreeze everything and train for some more epochs.\n",
    "            for layer in self.model.layers:\n",
    "                layer.trainable = True\n",
    "            \n",
    "            print \"Starting with whole model training...\"\n",
    "            self._fit_generator(\n",
    "                train_generator,\n",
    "                validation_generator,\n",
    "                batch_size,\n",
    "                num_whole_model_epochs,\n",
    "                best_model_path=best_model_path,\n",
    "                tensorboard_logs_path=tensorboard_logs_path,\n",
    "                tensorboard_logs_path_suffix='Whole',\n",
    "                reduce_learning_rate=True)\n",
    "            print \"Whole model training done\"\n",
    "        \n",
    "        return self\n",
    "    \n",
    "\n",
    "    def predict(self, X_batch):\n",
    "        return self.model.predict(X_batch)\n",
    "        \n",
    "    \n",
    "    def predict_generator(self, test_data_dir):\n",
    "        # As explained in https://github.com/fchollet/keras/issues/3896,\n",
    "        # there is an unresolved issue with predict_generator that causes\n",
    "        # errors when the number of samples is not divisible by the batch.\n",
    "        # size. To avoid this problem, let's use batch_size=1, for now.\n",
    "        batch_size = 1\n",
    "        \n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        test_generator = test_datagen.flow_from_directory(\n",
    "            test_data_dir,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=batch_size,\n",
    "            classes=self.classes.values(),\n",
    "            class_mode='categorical')\n",
    "        \n",
    "        \n",
    "        return self.model.predict_generator(\n",
    "            test_generator,\n",
    "            test_generator.n//batch_size)\n",
    "    \n",
    "    \n",
    "    def predict_classes(self, test_data_dir):\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        test_generator = test_datagen.flow_from_directory(\n",
    "            test_data_dir,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=batch_size,\n",
    "            classes=self.classes.values(),\n",
    "            class_mode='categorical')\n",
    "        \n",
    "        for X_batch, Y_batch in train_generator:\n",
    "            for i in range(len(Y_batch)):\n",
    "                show_sample(X_batch[i, :, :, :], Y_batch[i])\n",
    "            break\n",
    "        \n",
    "    \n",
    "    def evaluate(self, test_data_dir, batch_size=32, show_num_misclassified=0):\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        test_generator = test_datagen.flow_from_directory(\n",
    "            test_data_dir,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=batch_size,\n",
    "            classes=self.classes.values(),\n",
    "            class_mode='categorical')\n",
    "\n",
    "        num_gen_samples = 0\n",
    "        num_misclassified_shown = 0\n",
    "        all_pred_Y = np.array([], dtype='int64')\n",
    "        all_real_Y = np.array([], dtype='int64')\n",
    "        \n",
    "        for X_batch, Y_batch in test_generator:\n",
    "            Y_batch_preds = self.model.predict(X_batch)\n",
    "\n",
    "            Y_batch_real_classes = Y_batch.argmax(axis=-1)\n",
    "            Y_batch_pred_classes = Y_batch_preds.argmax(axis=-1)\n",
    "\n",
    "            all_real_Y = np.hstack((all_real_Y, Y_batch_real_classes))\n",
    "            all_pred_Y = np.hstack((all_pred_Y, Y_batch_pred_classes))\n",
    "\n",
    "            num_gen_samples += X_batch.shape[0]\n",
    "\n",
    "            if num_misclassified_shown < show_num_misclassified:\n",
    "                for i, y_real, y_pred in enumerate(zip(Y_batch_real_classes, Y_batch_pred_classes)):\n",
    "                    if y_real != y_pred:\n",
    "                        show_sample(X_batch[i], y_real, self.classes.values(), y_pred)\n",
    "                        num_misclassified_shown += 1\n",
    "\n",
    "            all_samples_generated = num_gen_samples >= test_generator.n\n",
    "            if all_samples_generated: break\n",
    "        \n",
    "        accuracy = float((all_real_Y == all_pred_Y).sum()) / len(all_real_Y)\n",
    "        return accuracy, all_pred_Y, all_real_Y\n",
    "\n",
    "    # Private methods.\n",
    "    def _fit_generator(self,\n",
    "                       train_generator,\n",
    "                       validation_generator,\n",
    "                       batch_size,\n",
    "                       num_epochs,\n",
    "                       best_model_path=None,\n",
    "                       tensorboard_logs_path=None,\n",
    "                       tensorboard_logs_path_suffix='',\n",
    "                       reduce_learning_rate=True):\n",
    "        # Recompile model before training to increase efficiency in\n",
    "        # case of frozen layers.\n",
    "        self._recompile_model()\n",
    "        \n",
    "        # Save the best model based on validation accuracy.\n",
    "        if best_model_path:\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                best_model_path,\n",
    "                monitor='val_acc',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                period=1\n",
    "            )\n",
    "        \n",
    "        # Log epochs information like loss and accuracy to review it\n",
    "        # afterwards using TensorBoard.\n",
    "        if tensorboard_logs_path:\n",
    "            if tensorboard_logs_path[-1] == '/':\n",
    "                tensorboard_logs_path = tensorboard_logs_path[:-1]\n",
    "            tensorboard = TensorBoard(\n",
    "                log_dir=tensorboard_logs_path+'-'+tensorboard_logs_path_suffix+'/')\n",
    "        \n",
    "        # Reduce learning rate i\n",
    "        if reduce_learning_rate:\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "        \n",
    "        self.model.fit_generator(\n",
    "            train_generator,\n",
    "            samples_per_epoch=train_generator.n//batch_size,\n",
    "            nb_epoch=num_epochs,\n",
    "            validation_data=validation_generator,\n",
    "            nb_val_samples=validation_generator.n//batch_size,\n",
    "            callbacks=[model_checkpoint, tensorboard, reduce_lr])\n",
    "        \n",
    "            \n",
    "    def _build_top_model(self,\n",
    "                         num_dense_layer_units=256,\n",
    "                         dropout=0.5,\n",
    "                         multiclass_output=False):\n",
    "        # We use the Functional API of Keras. \n",
    "        # https://keras.io/getting-started/functional-api-guide/\n",
    "        model = self.bottom_model\n",
    "        x = model.output\n",
    "        x = Flatten(input_shape=model.output_shape[1:])(x)\n",
    "        x = Dense(num_dense_layer_units, activation='relu')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        preds = Dense(\n",
    "            len(self.classes),\n",
    "            activation='sigmoid' if multiclass_output else 'softmax')(x)\n",
    "        self.num_top_layers = 4\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "\n",
    "    def _recompile_model(self):\n",
    "        if self.is_compiled != True:\n",
    "            raise Error(\"Model has to be compiled first.\")\n",
    "\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=self.optimizer,\n",
    "            metrics=['accuracy']\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models have been trained using the same set of hyperparameters:\n",
    "\n",
    "- Batch size of 16.\n",
    "- Adagrad [8] with a learning rate of 0.001 as optimizer.\n",
    "- 256 hidden units in the top layer with 0.5 dropout applied afterwards.\n",
    "- Categorical crossentropy as loss function.\n",
    "- 20 epochs of training for the top layers, and 80 more for the whole network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Training VGG16\"\n",
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(VGG16). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        categories_train_data_dir,\n",
    "        categories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/VGG16.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG16'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Training VGG19\"\n",
    "categorizer = Categorizer(CATEGORIES). \\\n",
    "    build_model(VGG19). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        categories_train_data_dir,\n",
    "        categories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/VGG19.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG19'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Training Xception\"\n",
    "categorizer = Categorizer(CATEGORIES). \\\n",
    "    build_model(Xception). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        categories_train_data_dir,\n",
    "        categories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/Xception.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/Xception'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Training InceptionV3\"\n",
    "categorizer = Categorizer(CATEGORIES). \\\n",
    "    build_model(InceptionV3). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        categories_train_data_dir,\n",
    "        categories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/InceptionV3.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/InceptionV3'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Training ResNet50\"\n",
    "categorizer = Categorizer(CATEGORIES). \\\n",
    "    build_model(ResNet50). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        categories_train_data_dir,\n",
    "        categories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/ResNet50.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/ResNet50'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a careful analysis of the training results, ResNet50 was decided to be the best architecture, given that its performance in the validation set was at the same level as Xception, but its training time was significantly lower. Therefore, a testing phase using the weights of the best model was done."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "resnet_categorizer = Categorizer(CATEGORIES). \\\n",
    "    build_model(ResNet50, weights='best_models/ResNet50.hdf5'). \\\n",
    "    compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(all_real_Y, all_pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnf_matrix.max()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=CATEGORIES.values(), normalize=True,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subcategories_train_data_dir = './subcategories-data/train'\n",
    "subcategories_validation_data_dir = './subcategories-data/validation'\n",
    "subcategories_test_data_dir = './subcategories-data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6133 images belonging to 13 classes.\n",
      "Found 1541 images belonging to 13 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "368/383 [===========================>..] - ETA: 0s - loss: 7.2776 - acc: 0.0027 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384/383 [==============================] - 12s - loss: 7.2063 - acc: 0.0026 - val_loss: 6.5475 - val_acc: 0.0000e+00\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "176/383 [============>.................] - ETA: 15s - loss: 6.6375 - acc: 0.0057"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/PIL/Image.py:857: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384/383 [==============================] - 28s - loss: 6.7998 - acc: 0.0104 - val_loss: 5.3413 - val_acc: 0.0208\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbeec44ee60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(VGG16, weights='best_models/VGG16.hdf5'). \\\n",
    "    change_top_model(SUBCATEGORIES, multiclass_output=True).compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/VGG16-subcats.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG16-subcats'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6133 images belonging to 13 classes.\n",
      "Found 1541 images belonging to 13 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 10s - loss: 2.5656 - acc: 0.1328 - val_loss: 2.2142 - val_acc: 0.1146\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 27s - loss: 2.8849 - acc: 0.1250 - val_loss: 2.2295 - val_acc: 0.0938\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbe8942dea8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(SUBCATEGORIES). \\\n",
    "    build_model(VGG16, weights='imagenet', multiclass_output=True). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/VGG16-subcats-imagenet.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG16-subcats-imagenet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6133 images belonging to 13 classes.\n",
      "Found 1541 images belonging to 13 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 12s - loss: 11.3504 - acc: 0.0104 - val_loss: 13.1778 - val_acc: 0.0104\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 32s - loss: 11.0728 - acc: 0.0182 - val_loss: 13.5093 - val_acc: 0.0000e+00\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbe871b76c8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(VGG19, weights='best_models/VGG19.hdf5'). \\\n",
    "    change_top_model(SUBCATEGORIES, multiclass_output=True).compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/VGG19-subcats.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG19-subcats'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 756 images belonging to 8 classes.\n",
      "Found 189 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 3s - loss: 1.0108 - acc: 0.6875 - val_loss: 3.2250e-04 - val_acc: 1.0000\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 6s - loss: 0.0865 - acc: 0.9375 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbeec47d1b8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(VGG19, weights='imagenet', multiclass_output=True). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/VGG19-subcats-imagenet.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG19-subcats-imagenet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6133 images belonging to 13 classes.\n",
      "Found 1541 images belonging to 13 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 16s - loss: 2.2988 - acc: 0.2135 - val_loss: 2.2479 - val_acc: 0.1354\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 25s - loss: 2.3106 - acc: 0.2318 - val_loss: 4.1637 - val_acc: 0.1354\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbe702b4dd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(ResNet50, weights='best_models/ResNet50.hdf5'). \\\n",
    "    change_top_model(SUBCATEGORIES, multiclass_output=True).compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/ResNet50-subcats.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/ResNet50-subcats'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 756 images belonging to 8 classes.\n",
      "Found 189 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 25s - loss: 1.5234 - acc: 0.6875 - val_loss: 1.3200 - val_acc: 1.0000\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 7s - loss: 0.2312 - acc: 1.0000 - val_loss: 0.1668 - val_acc: 1.0000\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbe702b4e60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(ResNet50, weights='imagenet', multiclass_output=True). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/ResNet50-subcats-imagenet.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/ResNet50-subcats-imagenet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6133 images belonging to 13 classes.\n",
      "Found 1541 images belonging to 13 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 43s - loss: 2.1470 - acc: 0.2708 - val_loss: 2.0793 - val_acc: 0.3333\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 49s - loss: 2.2092 - acc: 0.2812 - val_loss: 2.0577 - val_acc: 0.3646\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbe08095e18>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(Xception, weights='best_models/Xception.hdf5'). \\\n",
    "    change_top_model(SUBCATEGORIES, multiclass_output=True).compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/Xception-subcats.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/Xception-subcats'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 756 images belonging to 8 classes.\n",
      "Found 189 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 38s - loss: 0.9838 - acc: 0.5833 - val_loss: 1.7881e-07 - val_acc: 1.0000\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 14s - loss: 0.1130 - acc: 0.9583 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbeed992290>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(Xception, weights='imagenet', multiclass_output=True). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/Xception-subcats-imagenet.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/Xception-subcats-imagenet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6133 images belonging to 13 classes.\n",
      "Found 1541 images belonging to 13 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 114s - loss: 2.9726 - acc: 0.0156 - val_loss: 2.6039 - val_acc: 0.0104\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 38s - loss: 3.8170 - acc: 0.0443 - val_loss: 4.1145 - val_acc: 0.1354\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbdc389d830>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(InceptionV3, weights='best_models/InceptionV3.hdf5'). \\\n",
    "    change_top_model(SUBCATEGORIES, multiclass_output=True).compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/InceptionV3-subcats.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/InceptionV3-subcats'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 756 images belonging to 8 classes.\n",
      "Found 189 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 153s - loss: 1.9215 - acc: 0.5625 - val_loss: 0.0827 - val_acc: 1.0000\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 16s - loss: 1.7190 - acc: 0.8542 - val_loss: 15.1107 - val_acc: 0.0625\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbdc389d050>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(InceptionV3, weights='imagenet', multiclass_output=True). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/InceptionV3-subcats-imagenet.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/InceptionV3-subcats-imagenet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "img_path = '/Users/Josevi/product_images/photo/9760-1.jpg'\n",
    "img = load_img(img_path, target_size=(224, 224))\n",
    "x = img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "imgplot = plt.imshow(mpimg.imread(img_path))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%time\n",
    "preds = model.predict(x)\n",
    "print preds\n",
    "#print('Predicted:', decode_predictions(preds))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for X_batch, Y_batch in train_generator:\n",
    "    for i in range(len(Y_batch)):\n",
    "        show_sample(X_batch[i, :, :, :], Y_batch[i])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Erin Hogg, _\"Lead Generation: How one additional form field decreased conversions 11% [Lead Gen Summit 2013 live test].\"_ 2013. URL: https://www.marketingsherpa.com/article/case-study/how-one-additional-form-field \n",
    "2. Olga Russakovsky\\*, Jia Deng\\*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) _\"ImageNet Large Scale Visual Recognition Challenge.\"_ IJCV, 2015.\n",
    "3. François Chollet et al. _\"Keras.\"_ 2015. URL: https://github.com/fchollet/keras\n",
    "4. K. Simonyan, A. Zisserman. _\"Very Deep Convolutional Networks for Large-Scale Image Recognition.\"_ 2014.\n",
    "5. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. _\"Deep Residual Learning for Image Recognition.\"_ 2015.\n",
    "6. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. _\"Going Deeper with Convolutions.\"_ 2014.\n",
    "7. François Chollet. _\"Xception: Deep Learning with Depthwise Separable Convolutions.\"_ 2016.\n",
    "8. John Duchi, Elad Hazan, Yoram Singer. _\"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.\"_ 2011.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 - DLforCV",
   "language": "python",
   "name": "python2-dlforcv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
