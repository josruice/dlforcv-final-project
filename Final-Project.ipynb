{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frictionless Product Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author:** Jose Vicente Ruiz Cepeda (jr3660)\n",
    "- **Course:** COMS W 4995 - Deep Learning for Computer Vision\n",
    "- **Assignment**: Final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain some of the context of the problem here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras # Keras 1.2.2 assumed.\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications import Xception\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Required to avoid errors with the images.\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    0: 'photo',\n",
    "    1: 'electronics',\n",
    "    2: 'events',\n",
    "    3: 'instruments',\n",
    "    4: 'tools',\n",
    "    5: 'sports',\n",
    "    6: 'caravans',\n",
    "    7: 'others'\n",
    "}\n",
    "\n",
    "SUBCATEGORIES = {\n",
    "    0:  '360',\n",
    "    1:  'action_cameras',\n",
    "    2:  'drones',\n",
    "    3:  'instant_cameras',\n",
    "    4:  'kits',\n",
    "    5:  'lenses',\n",
    "    6:  'lighting',\n",
    "    7:  'others',\n",
    "    8:  'photo_cameras',\n",
    "    9:  'projectors',\n",
    "    10: 'sound',\n",
    "    11: 'supports',\n",
    "    12: 'video_cameras'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modified from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=60)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"%d%%\" % int(cm[i, j]*100),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_sample(X, y, class_names, prediction=-1):\n",
    "    im = X\n",
    "    print y\n",
    "    #y = np.flip(y, axis=0)\n",
    "    y_label = class_names[y]\n",
    "    plt.imshow(im)\n",
    "    if prediction >= 0:\n",
    "        plt.title(\"Class = %s, Predict = %s\" % (y_label, class_names[prediction]))\n",
    "    else:\n",
    "        plt.title(\"Class = %s\" % (y_label))\n",
    "\n",
    "    plt.axis('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I - Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = './data/train'\n",
    "validation_data_dir = './data/validation'\n",
    "test_data_dir = './data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Categorizer:\n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "        \n",
    "    def build_model(self, \n",
    "                    ImagenetModel=ResNet50,\n",
    "                    weights='imagenet',\n",
    "                    num_dense_layer_units=256,\n",
    "                    dropout=0.5,\n",
    "                    multiclass_output=False,\n",
    "                    verbose=False):\n",
    "        # Keras models are functions, not classes, so we have to check which one is it like this.\n",
    "        if ImagenetModel.func_name == 'InceptionV3' or ImagenetModel.func_name == 'Xception':\n",
    "            self.img_width, self.img_height = 299, 299\n",
    "        else:\n",
    "            self.img_width, self.img_height = 224, 224\n",
    "\n",
    "        # First, let's load the model with ImageNet weights and without the top layer.\n",
    "        # This will take some time the first time, since the weights have to be downloaded.\n",
    "        model_weights = weights if weights == 'imagenet' else None\n",
    "        model = ImagenetModel(\n",
    "            weights=model_weights,\n",
    "            include_top=False,\n",
    "            input_shape=(self.img_width, self.img_height, 3)\n",
    "        )\n",
    "        self.bottom_model = model\n",
    "\n",
    "        # Now, let's create the top layers adapted to our problem. \n",
    "        preds = self._build_top_model(\n",
    "            num_dense_layer_units=num_dense_layer_units,\n",
    "            dropout=dropout,\n",
    "            multiclass_output=multiclass_output)\n",
    "\n",
    "        # Combine both models to get the final one.\n",
    "        self.model = Model(model.input, preds)\n",
    "\n",
    "        if verbose:\n",
    "            self.model.summary()\n",
    "            \n",
    "        # If weights is a string different from ImageNet, path to weights is assumed.\n",
    "        if os.path.exists(weights):\n",
    "            self.model.load_weights(weights)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def change_top_model(self,\n",
    "                         new_classes,\n",
    "                         num_dense_layer_units=256,\n",
    "                         dropout=0.5,\n",
    "                         multiclass_output=False,\n",
    "                         verbose=False):\n",
    "        self.classes = new_classes\n",
    "        model = self.bottom_model\n",
    "        preds = self._build_top_model(\n",
    "            num_dense_layer_units=num_dense_layer_units,\n",
    "            dropout=dropout,\n",
    "            multiclass_output=multiclass_output)\n",
    "\n",
    "        self.model = Model(model.input, preds)\n",
    "        if verbose:\n",
    "            self.model.summary()\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def _build_top_model(self,\n",
    "                         num_dense_layer_units=256,\n",
    "                         dropout=0.5,\n",
    "                         multiclass_output=False):\n",
    "        # We use the Functional API of Keras. \n",
    "        # https://keras.io/getting-started/functional-api-guide/\n",
    "        model = self.bottom_model\n",
    "        x = model.output\n",
    "        x = Flatten(input_shape=model.output_shape[1:])(x)\n",
    "        x = Dense(num_dense_layer_units, activation='relu')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        preds = Dense(\n",
    "            len(self.classes),\n",
    "            activation='sigmoid' if multiclass_output else 'softmax')(x)\n",
    "        self.num_top_layers = 4\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def compile_model(self, optimizer=optimizers.Adagrad(lr=0.001)):\n",
    "        self.is_compiled = True\n",
    "        self.optimizer = optimizer\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def _recompile_model(self):\n",
    "        if self.is_compiled != True:\n",
    "            raise Error(\"Model has to be compiled first.\")\n",
    "\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=self.optimizer,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    def fine_tune(self,\n",
    "                  train_data_dir,\n",
    "                  validation_data_dir,\n",
    "                  batch_size=16,\n",
    "                  num_only_top_epochs=10,\n",
    "                  num_whole_model_epochs=40,\n",
    "                  best_model_path=None,\n",
    "                  tensorboard_logs_path=None,\n",
    "                  reduce_learning_rate=True):\n",
    "        \n",
    "        # Augmentation configurations for training and validation.\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "        validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            train_data_dir,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=batch_size,\n",
    "            classes=self.classes.values(),\n",
    "            class_mode='categorical')\n",
    "\n",
    "        validation_generator = validation_datagen.flow_from_directory(\n",
    "            validation_data_dir,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=batch_size,\n",
    "            classes=self.classes.values(),\n",
    "            class_mode='categorical')\n",
    "        \n",
    "        if num_only_top_epochs > 0:\n",
    "            # Freeze everything except the top layers, before training.\n",
    "            for layer in self.model.layers[:-self.num_top_layers]:\n",
    "                layer.trainable = False \n",
    "\n",
    "            print \"Starting with top layers training...\"\n",
    "            self._fit_generator(\n",
    "                train_generator,\n",
    "                validation_generator,\n",
    "                batch_size,\n",
    "                num_only_top_epochs,\n",
    "                best_model_path=best_model_path,\n",
    "                tensorboard_logs_path=tensorboard_logs_path,\n",
    "                tensorboard_logs_path_suffix='Top',\n",
    "                reduce_learning_rate=True)\n",
    "            print \"Top layers training done.\"\n",
    "            \n",
    "        if num_whole_model_epochs > 0:\n",
    "            # Unfreeze everything and train for some more epochs.\n",
    "            for layer in self.model.layers:\n",
    "                layer.trainable = True\n",
    "            \n",
    "            print \"Starting with whole model training...\"\n",
    "            self._fit_generator(\n",
    "                train_generator,\n",
    "                validation_generator,\n",
    "                batch_size,\n",
    "                num_whole_model_epochs,\n",
    "                best_model_path=best_model_path,\n",
    "                tensorboard_logs_path=tensorboard_logs_path,\n",
    "                tensorboard_logs_path_suffix='Whole',\n",
    "                reduce_learning_rate=True)\n",
    "            print \"Whole model training done\"\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_batch):\n",
    "        return self.model.predict(X_batch)\n",
    "        \n",
    "    def predict_generator(self, test_data_dir):\n",
    "        # As explained in https://github.com/fchollet/keras/issues/3896,\n",
    "        # there is an unresolved issue with predict_generator that causes\n",
    "        # errors when the number of samples is not divisible by the batch.\n",
    "        # size. To avoid this problem, let's use batch_size=1, for now.\n",
    "        batch_size = 1\n",
    "        \n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        test_generator = test_datagen.flow_from_directory(\n",
    "            test_data_dir,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=batch_size,\n",
    "            classes=self.classes.values(),\n",
    "            class_mode='categorical')\n",
    "        \n",
    "        \n",
    "        return self.model.predict_generator(\n",
    "            test_generator,\n",
    "            test_generator.n//batch_size)\n",
    "    \n",
    "    def predict_classes(self, test_data_dir):\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        test_generator = test_datagen.flow_from_directory(\n",
    "            test_data_dir,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=batch_size,\n",
    "            classes=self.classes.values(),\n",
    "            class_mode='categorical')\n",
    "        \n",
    "        for X_batch, Y_batch in train_generator:\n",
    "            for i in range(len(Y_batch)):\n",
    "                show_sample(X_batch[i, :, :, :], Y_batch[i])\n",
    "            break\n",
    "        \n",
    "    \n",
    "    def evaluate(self, test_data_dir, batch_size=32):\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        test_generator = test_datagen.flow_from_directory(\n",
    "            test_data_dir,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=batch_size,\n",
    "            classes=self.classes.values(),\n",
    "            class_mode='categorical')\n",
    "        \n",
    "        return self.model.evaluate_generator(\n",
    "            test_generator,\n",
    "            test_generator.n//batch_size)\n",
    "    \n",
    "    # Private methods.\n",
    "    def _fit_generator(self,\n",
    "                       train_generator,\n",
    "                       validation_generator,\n",
    "                       batch_size,\n",
    "                       num_epochs,\n",
    "                       best_model_path=None,\n",
    "                       tensorboard_logs_path=None,\n",
    "                       tensorboard_logs_path_suffix='',\n",
    "                       reduce_learning_rate=True):\n",
    "        # Recompile model before training to increase efficiency in\n",
    "        # case of frozen layers.\n",
    "        self._recompile_model()\n",
    "        \n",
    "        # Save the best model based on validation accuracy.\n",
    "        if best_model_path:\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                best_model_path,\n",
    "                monitor='val_acc',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                period=1\n",
    "            )\n",
    "        \n",
    "        # Log epochs information like loss and accuracy to review it\n",
    "        # afterwards using TensorBoard.\n",
    "        if tensorboard_logs_path:\n",
    "            if tensorboard_logs_path[-1] == '/':\n",
    "                tensorboard_logs_path = tensorboard_logs_path[:-1]\n",
    "            tensorboard = TensorBoard(\n",
    "                log_dir=tensorboard_logs_path+'-'+tensorboard_logs_path_suffix+'/')\n",
    "        \n",
    "        # Reduce learning rate i\n",
    "        if reduce_learning_rate:\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "        \n",
    "        self.model.fit_generator(\n",
    "            train_generator,\n",
    "            samples_per_epoch=train_generator.n//batch_size,\n",
    "            nb_epoch=num_epochs,\n",
    "            validation_data=validation_generator,\n",
    "            nb_val_samples=validation_generator.n//batch_size,\n",
    "            callbacks=[model_checkpoint, tensorboard, reduce_lr])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "print \"Training VGG16\"\n",
    "categorizer = Categorizer(CATEGORIES). \\\n",
    "    build_model(VGG16). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        train_data_dir,\n",
    "        validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/VGG16.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG16'\n",
    "    )\n",
    "del categorizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "print \"Training VGG19\"\n",
    "categorizer = Categorizer(CATEGORIES). \\\n",
    "    build_model(VGG19). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        train_data_dir,\n",
    "        validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/VGG19.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG19'\n",
    "    )\n",
    "del categorizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "print \"Training Xception\"\n",
    "categorizer = Categorizer(CATEGORIES). \\\n",
    "    build_model(Xception). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        train_data_dir,\n",
    "        validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/Xception.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/Xception'\n",
    "    )\n",
    "del categorizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "print \"Training InceptionV3\"\n",
    "categorizer = Categorizer(CATEGORIES). \\\n",
    "    build_model(InceptionV3). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        train_data_dir,\n",
    "        validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/InceptionV3.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/InceptionV3'\n",
    "    )\n",
    "del categorizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "print \"Training ResNet50\"\n",
    "categorizer = Categorizer(CATEGORIES). \\\n",
    "    build_model(ResNet50). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        train_data_dir,\n",
    "        validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/ResNet50.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/ResNet50'\n",
    "    )\n",
    "del categorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "resnet_categorizer = Categorizer(CATEGORIES). \\\n",
    "    build_model(ResNet50, weights='best_models/ResNet50.hdf5'). \\\n",
    "    compile_model()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(resnet_categorizer.img_width, resnet_categorizer.img_height),\n",
    "    batch_size=32,\n",
    "    classes=CATEGORIES.values(),\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "%%time\n",
    "num_gen_samples = 0\n",
    "all_pred_Y = np.array([], dtype='int64')\n",
    "all_real_Y = np.array([], dtype='int64')\n",
    "for X_batch, Y_batch in test_generator:\n",
    "    Y_batch_preds = resnet_categorizer.model.predict(X_batch)\n",
    "    \n",
    "    Y_batch_real_classes = Y_batch.argmax(axis=-1)\n",
    "    Y_batch_pred_classes = Y_batch_preds.argmax(axis=-1)\n",
    "    \n",
    "    all_real_Y = np.hstack((all_real_Y, Y_batch_real_classes))\n",
    "    all_pred_Y = np.hstack((all_pred_Y, Y_batch_pred_classes))\n",
    "    \n",
    "    num_gen_samples += X_batch.shape[0]\n",
    "    \n",
    "    #for i in range(len(Y_batch)):\n",
    "    #    if Y_batch_pred_classes[i] != Y_batch_real_classes[i]:\n",
    "    #        show_sample(X_batch[i, :, :, :],Y_batch_real_classes[i], Y_batch_pred_classes[i], )\n",
    "    \n",
    "    all_samples_generated = num_gen_samples >= test_generator.n\n",
    "    if all_samples_generated: break\n",
    "    #if num_gen_samples > 200: break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(all_real_Y == all_pred_Y).sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(all_real_Y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cnf_matrix = confusion_matrix(all_real_Y, all_pred_Y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cnf_matrix.max()/2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=CATEGORIES.values(), normalize=True,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subcategories_train_data_dir = './subcategories-data/train'\n",
    "subcategories_validation_data_dir = './subcategories-data/validation'\n",
    "subcategories_test_data_dir = './subcategories-data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6133 images belonging to 13 classes.\n",
      "Found 1541 images belonging to 13 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "368/383 [===========================>..] - ETA: 0s - loss: 7.2776 - acc: 0.0027 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384/383 [==============================] - 12s - loss: 7.2063 - acc: 0.0026 - val_loss: 6.5475 - val_acc: 0.0000e+00\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "176/383 [============>.................] - ETA: 15s - loss: 6.6375 - acc: 0.0057"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/PIL/Image.py:857: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384/383 [==============================] - 28s - loss: 6.7998 - acc: 0.0104 - val_loss: 5.3413 - val_acc: 0.0208\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbeec44ee60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(VGG16, weights='best_models/VGG16.hdf5'). \\\n",
    "    change_top_model(SUBCATEGORIES, multiclass_output=True).compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/VGG16-subcats.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG16-subcats'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6133 images belonging to 13 classes.\n",
      "Found 1541 images belonging to 13 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 10s - loss: 2.5656 - acc: 0.1328 - val_loss: 2.2142 - val_acc: 0.1146\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 27s - loss: 2.8849 - acc: 0.1250 - val_loss: 2.2295 - val_acc: 0.0938\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbe8942dea8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(SUBCATEGORIES). \\\n",
    "    build_model(VGG16, weights='imagenet', multiclass_output=True). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/VGG16-subcats-imagenet.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG16-subcats-imagenet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6133 images belonging to 13 classes.\n",
      "Found 1541 images belonging to 13 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 12s - loss: 11.3504 - acc: 0.0104 - val_loss: 13.1778 - val_acc: 0.0104\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 32s - loss: 11.0728 - acc: 0.0182 - val_loss: 13.5093 - val_acc: 0.0000e+00\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbe871b76c8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(VGG19, weights='best_models/VGG19.hdf5'). \\\n",
    "    change_top_model(SUBCATEGORIES, multiclass_output=True).compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/VGG19-subcats.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG19-subcats'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 756 images belonging to 8 classes.\n",
      "Found 189 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 3s - loss: 1.0108 - acc: 0.6875 - val_loss: 3.2250e-04 - val_acc: 1.0000\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 6s - loss: 0.0865 - acc: 0.9375 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbeec47d1b8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(VGG19, weights='imagenet', multiclass_output=True). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/VGG19-subcats-imagenet.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG19-subcats-imagenet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6133 images belonging to 13 classes.\n",
      "Found 1541 images belonging to 13 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 16s - loss: 2.2988 - acc: 0.2135 - val_loss: 2.2479 - val_acc: 0.1354\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 25s - loss: 2.3106 - acc: 0.2318 - val_loss: 4.1637 - val_acc: 0.1354\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbe702b4dd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(ResNet50, weights='best_models/ResNet50.hdf5'). \\\n",
    "    change_top_model(SUBCATEGORIES, multiclass_output=True).compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/ResNet50-subcats.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/ResNet50-subcats'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 756 images belonging to 8 classes.\n",
      "Found 189 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 25s - loss: 1.5234 - acc: 0.6875 - val_loss: 1.3200 - val_acc: 1.0000\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 7s - loss: 0.2312 - acc: 1.0000 - val_loss: 0.1668 - val_acc: 1.0000\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbe702b4e60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(ResNet50, weights='imagenet', multiclass_output=True). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/ResNet50-subcats-imagenet.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/ResNet50-subcats-imagenet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6133 images belonging to 13 classes.\n",
      "Found 1541 images belonging to 13 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 43s - loss: 2.1470 - acc: 0.2708 - val_loss: 2.0793 - val_acc: 0.3333\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 49s - loss: 2.2092 - acc: 0.2812 - val_loss: 2.0577 - val_acc: 0.3646\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbe08095e18>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(Xception, weights='best_models/Xception.hdf5'). \\\n",
    "    change_top_model(SUBCATEGORIES, multiclass_output=True).compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/Xception-subcats.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/Xception-subcats'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 756 images belonging to 8 classes.\n",
      "Found 189 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 38s - loss: 0.9838 - acc: 0.5833 - val_loss: 1.7881e-07 - val_acc: 1.0000\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 14s - loss: 0.1130 - acc: 0.9583 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbeed992290>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(Xception, weights='imagenet', multiclass_output=True). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/Xception-subcats-imagenet.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/Xception-subcats-imagenet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6133 images belonging to 13 classes.\n",
      "Found 1541 images belonging to 13 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 114s - loss: 2.9726 - acc: 0.0156 - val_loss: 2.6039 - val_acc: 0.0104\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "384/383 [==============================] - 38s - loss: 3.8170 - acc: 0.0443 - val_loss: 4.1145 - val_acc: 0.1354\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbdc389d830>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(InceptionV3, weights='best_models/InceptionV3.hdf5'). \\\n",
    "    change_top_model(SUBCATEGORIES, multiclass_output=True).compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/InceptionV3-subcats.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/InceptionV3-subcats'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 756 images belonging to 8 classes.\n",
      "Found 189 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 153s - loss: 1.9215 - acc: 0.5625 - val_loss: 0.0827 - val_acc: 1.0000\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/1\n",
      "48/47 [==============================] - 16s - loss: 1.7190 - acc: 0.8542 - val_loss: 15.1107 - val_acc: 0.0625\n",
      "Whole model training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Categorizer instance at 0x7fbdc389d050>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Categorizer(CATEGORIES). \\\n",
    "    build_model(InceptionV3, weights='imagenet', multiclass_output=True). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        subcategories_train_data_dir,\n",
    "        subcategories_validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=50,\n",
    "        best_model_path='best_models/InceptionV3-subcats-imagenet.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/InceptionV3-subcats-imagenet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "img_path = '/Users/Josevi/product_images/photo/9760-1.jpg'\n",
    "img = load_img(img_path, target_size=(224, 224))\n",
    "x = img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "imgplot = plt.imshow(mpimg.imread(img_path))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%time\n",
    "preds = model.predict(x)\n",
    "print preds\n",
    "#print('Predicted:', decode_predictions(preds))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for X_batch, Y_batch in train_generator:\n",
    "    for i in range(len(Y_batch)):\n",
    "        show_sample(X_batch[i, :, :, :], Y_batch[i])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
