{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision:  Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Science: COMS W 4995 004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal: March 21, 2017\n",
    "### Presentations: April 20, 25, and 27, 2017\n",
    "### Report: April 27, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final project is one of the most important and, hopefully, exciting components of the course. You will have the opportunity to develop a deep learning system of your own choosing. \n",
    "You are free to select whatever framework (Tensorflow, Theano, Caffe) or wrapper (Keras, TFLearn) you like, but you need create a report on your project in a Jupyter notebook. You are also free build on publically available models and code, but your report must clearly give attribution for the work of others and must clearly delineate your contributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project description should include the title of the project, participants, a description of the objectives of the project, and a plan for how the project will be completed. The description of the objectives should include modest predictions of the success of the project. The plan for completion should include a description of the training data and how it will be obtained, a discussion of what deep learning framework will be used and why, and a rough description of the planned network architecture.\n",
    "\n",
    "You are permitted to work together on a project in groups of two or three, but group size must not exceed three participants.  For group projects there must be a clearly delineated division of labor: you should state in the project description and project report who was responsible for which portion of the project. Each student must hand in a separate report. (Students will not necessarily get the same grade for the same project.)\n",
    "\n",
    "You should mention whether you are simply re-implementing what others have done before but applying to new data or whether you are attempting to do something new to the best of your knowledge. Creative and original projects will be judged more kindly than those that are rehashing something in the existing literature. And projects that include a component in which data is acquired/curated into training and validation sets will be veiwed more favorably than those that simply download an existing data set such as CIFAR-100.\n",
    "\n",
    "As this is a computer vision course it is expected that your data will be visual, but excpetions might be made if the student is enthusiatic and persuasive enough. The most straightforward project would be to build a system that classifies images into catogories. A more difficult project might be to build a system that detects and localizes a type of object within an image. A still more complicated project might involve joining a ConvNet with an LSTM for a problem (like image captioning) that requires vision and language. But again, creative and originnal projects will be judged more kindly.  \n",
    "\n",
    "It is important to scope your project so that you get some working results. Project reports that say \"I tried this and this but nothing seemed to work...\" are discouraged. \n",
    "Above all, you should demonstrate end-to-end fluency in the basics of deep learning. \n",
    "\n",
    "I cannot wait to see the results. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To allow students to present their work in three class periods, each student will have only 3 minutes, not a second more. We will be strict about the timing, so you should practice your presentation. The key here is to get across three things: what you did, how you did it, and how well it worked. Students working in groups of two will get 6 minutes and groups of three will get 9 minutes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report should be done as a Jupyter Notebook. The report should be a complete description of the objectives of the work, the methods used to solve the problem, experimental evidence of a working system, the code, and clear delineation of what you have done vs. what you are leveraging that others have done. If you have used the work of others YOU MUST INCLUDE ATTRIBUTION by citing this work inline and as part of a \"bibliography\" at the end. You should describe what worked, what did not, and why. If you are working in a group you need to submit your own report and this report should be clear about what your individual contribution was.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title of the project: **Frictionless product categorization**\n",
    "- Participant: Jose Vicente Ruiz Cepeda (jr3660)\n",
    "- Context: I am the cofounder of Relendo, a peer to peer rental platform where users upload products that other users can rent (think Airbnb but with goods like high-end cameras, tools, sports equipment...). One of the things users have to do when they upload a product is to take a picture of it, and then select the category (and, sometimes, subcategory) it belongs to. A Neural Network could completely avoid this step, if it were able to select the category just based on the image(s) uploaded by the user. Although this would be enough to create value, the Neural Network could also recognize the object in the images and give this information to another system that would return information of similar products that has been uploaded and that are successful, so that the user can have a reference in terms of wording for the title/description, price per day, etc.\n",
    "- Objectives:\n",
    "  - 1: Build a Neural Network that is able to classify a product image in its correct category 80% of the times.\n",
    "  - 2: Build a Neural Network that is able to detect the product in the image with a 50% acurracy.\n",
    "  - 3 (probably for the future): Build a Neural Network that is able to learn the features of the different products from their images.\n",
    "- Plan:\n",
    "  - 1: First, extract, clean and curate the dataset of the images already uploaded by users. As of today (March 21st) it is composed of almost 12,000 products with more than 18,000 images. Then, design and implement a Neural Network that is able to categorize an image in one of the eight available categories in Relendo: photography and video, sports, electronics, tools, events, musical instruments, caravans, and others (the default one), with an accuracy of, at least, 80% in the test set.\n",
    "  - 2: Create another dataset using the titles of the products splitted by spaces as labels for the images. Then, design and implement a Neural Network that is able to infer this labels, which will represent the specific products from the images. This information could later be used to help the user with the uploading process.\n",
    "  - 3 (probably for the future): design and implement a new network that will perform Unsupervised Feature Learning over the images, so that the features can later be used to find clusters of products. This would make easy to detect when a certain type of product is frequent enough to be its own category/subcategory.\n",
    "- Dataset: the set of images uploaded by the users along with their products information to www.relendo.com.\n",
    "- Deep Learning framework: I will start using Keras because it is easier to use, with TensorFlow as backend, since it has the advantage of being easy to deploy in production environments.\n",
    "- Deep Learning architecture: I will have to try different architectures, but I am pretty sure that all of them will be sequential and will contain several convolutional layers that will connect to dense layers by the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications import Xception # TensorFlow ONLY\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RELENDO_DATASET = 0\n",
    "CATS_DOGS_DATASET = 1\n",
    "\n",
    "DATASET = RELENDO_DATASET\n",
    "MANUAL_VGG16 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if DATASET == RELENDO_DATASET:\n",
    "    nb_classes = 8\n",
    "    class_name = {\n",
    "        0: 'photo',\n",
    "        1: 'electronics',\n",
    "        2: 'events',\n",
    "        3: 'instruments',\n",
    "        4: 'tools',\n",
    "        5: 'sports',\n",
    "        6: 'caravans',\n",
    "        7: 'others'\n",
    "    }\n",
    "else:\n",
    "    nb_classes = 2\n",
    "    class_name = {\n",
    "        0: 'cats',\n",
    "        1: 'dogs'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "if DATASET == RELENDO_DATASET:\n",
    "    train_data_dir = './data/train'\n",
    "    validation_data_dir = './data/validation'\n",
    "    nb_train_samples = 15478\n",
    "    nb_validation_samples = 3876\n",
    "elif DATASET == CATS_DOGS_DATASET:\n",
    "    train_data_dir = './cats-dogs-data/train'\n",
    "    validation_data_dir = './cats-dogs-data/validation'\n",
    "    nb_train_samples = 20000\n",
    "    nb_validation_samples = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15478 images belonging to 8 classes.\n",
      "Found 3876 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "if DATASET == RELENDO_DATASET:\n",
    "    class_mode = 'categorical'\n",
    "elif DATASET == CATS_DOGS_DATASET:\n",
    "    class_mode = 'binary'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        classes=class_name.values(),\n",
    "        class_mode=class_mode)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=16,\n",
    "        classes=class_name.values(),\n",
    "        class_mode=class_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(img_width, img_height, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg16(framework='tf'):\n",
    "\n",
    "    if framework == 'th':\n",
    "        # build the VGG16 network in Theano weight ordering mode\n",
    "        backend.set_image_dim_ordering('th')\n",
    "    else:\n",
    "        # build the VGG16 network in Tensorflow weight ordering mode\n",
    "        backend.set_image_dim_ordering('tf')\n",
    "        \n",
    "    model = Sequential()\n",
    "    if framework == 'th':\n",
    "        model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "    else:\n",
    "        model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))\n",
    "        \n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "if MANUAL_VGG16:\n",
    "    # path to the model weights files.\n",
    "    weights_path = '../HW5/vgg16_weights.h5'\n",
    "    th_model = build_vgg16('th')\n",
    "\n",
    "    # load the weights of the VGG16 networks\n",
    "    # (trained on ImageNet, won the ILSVRC competition in 2014)\n",
    "    # note: when there is a complete match between your model definition\n",
    "    # and your weight savefile, you can simply call model.load_weights(filename)\n",
    "    assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n",
    "    f = h5py.File(weights_path)\n",
    "    for k in range(f.attrs['nb_layers']):\n",
    "        if k >= len(th_model.layers):\n",
    "            # we don't look at the last (fully-connected) layers in the savefile\n",
    "            break\n",
    "        g = f['layer_{}'.format(k)]\n",
    "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "        th_model.layers[k].set_weights(weights)\n",
    "    f.close()\n",
    "    print('Model loaded.')\n",
    "\n",
    "    tf_model = build_vgg16('tf')\n",
    "\n",
    "    # transfer weights from th_model to tf_model\n",
    "    for th_layer, tf_layer in zip(th_model.layers, tf_model.layers):\n",
    "        if th_layer.__class__.__name__ == 'Convolution2D':\n",
    "            kernel, bias = th_layer.get_weights()\n",
    "            kernel = np.transpose(kernel, (2, 3, 1, 0))\n",
    "            tf_layer.set_weights([kernel, bias])\n",
    "        else:\n",
    "            tf_layer.set_weights(tf_layer.get_weights())\n",
    "    \n",
    "    model = tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional mode\n",
    "x = model.output\n",
    "x = Flatten(input_shape=model.output_shape[1:])(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "if DATASET == RELENDO_DATASET:\n",
    "    preds = Dense(8, activation='softmax')(x)\n",
    "elif DATASET == CATS_DOGS_DATASET:\n",
    "    preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "final_model = Model(model.input, preds)\n",
    "# print (final_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in final_model.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (final_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "\n",
    "if DATASET == RELENDO_DATASET:\n",
    "    loss = 'categorical_crossentropy'\n",
    "elif DATASET == CATS_DOGS_DATASET:\n",
    "    loss = 'binary_crossentropy'\n",
    "\n",
    "final_model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=optimizers.Adagrad(lr=0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "480/483 [============================>.] - ETA: 1s - loss: 1.9805 - acc: 0.3250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Josevi/Dropbox/Coding/DLforCV/venv/lib/python2.7/site-packages/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/483 [==============================] - 216s - loss: 1.9621 - acc: 0.3286 - val_loss: 2.3011 - val_acc: 0.2344\n",
      "Epoch 2/2\n",
      "496/483 [==============================] - 215s - loss: 1.8341 - acc: 0.3730 - val_loss: 2.0398 - val_acc: 0.2656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x146bc6d90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "# Keras 2\n",
    "#final_model.fit_generator(\n",
    "#    train_generator,\n",
    "#    steps_per_epoch=1000,\n",
    "#    epochs=epochs,\n",
    "#    validation_data=validation_generator,\n",
    "#    validation_steps=100\n",
    "#)\n",
    "\n",
    "\n",
    "# Keras 1\n",
    "final_model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=(nb_train_samples // batch_size),\n",
    "    nb_epoch=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=(nb_validation_samples // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "496/483 [==============================] - 216s - loss: 1.5829 - acc: 0.4355 - val_loss: 1.9680 - val_acc: 0.2969\n",
      "Epoch 2/20\n",
      "496/483 [==============================] - 214s - loss: 1.5250 - acc: 0.4577 - val_loss: 1.9727 - val_acc: 0.2266\n",
      "Epoch 3/20\n",
      "480/483 [============================>.] - ETA: 1s - loss: 1.4645 - acc: 0.4813"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Josevi/Dropbox/Coding/DLforCV/venv/lib/python2.7/site-packages/PIL/Image.py:885: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/483 [==============================] - 213s - loss: 1.4676 - acc: 0.4798 - val_loss: 1.9848 - val_acc: 0.1875\n",
      "Epoch 4/20\n",
      "496/483 [==============================] - 211s - loss: 1.4507 - acc: 0.4698 - val_loss: 1.9858 - val_acc: 0.2109\n",
      "Epoch 5/20\n",
      "496/483 [==============================] - 212s - loss: 1.4545 - acc: 0.4758 - val_loss: 1.8834 - val_acc: 0.2422\n",
      "Epoch 6/20\n",
      "496/483 [==============================] - 210s - loss: 1.3625 - acc: 0.4960 - val_loss: 1.8772 - val_acc: 0.2422\n",
      "Epoch 7/20\n",
      "496/483 [==============================] - 210s - loss: 1.4823 - acc: 0.4556 - val_loss: 1.8565 - val_acc: 0.2109\n",
      "Epoch 8/20\n",
      "496/483 [==============================] - 210s - loss: 1.4645 - acc: 0.4758 - val_loss: 1.9279 - val_acc: 0.1719\n",
      "Epoch 9/20\n",
      "496/483 [==============================] - 209s - loss: 1.4217 - acc: 0.4899 - val_loss: 1.9161 - val_acc: 0.2656\n",
      "Epoch 10/20\n",
      "496/483 [==============================] - 210s - loss: 1.3992 - acc: 0.4879 - val_loss: 1.9459 - val_acc: 0.2656\n",
      "Epoch 11/20\n",
      "496/483 [==============================] - 209s - loss: 1.3646 - acc: 0.5040 - val_loss: 1.7611 - val_acc: 0.3203\n",
      "Epoch 12/20\n",
      "496/483 [==============================] - 210s - loss: 1.4058 - acc: 0.4839 - val_loss: 1.7499 - val_acc: 0.3125\n",
      "Epoch 13/20\n",
      "496/483 [==============================] - 208s - loss: 1.3303 - acc: 0.5040 - val_loss: 1.6932 - val_acc: 0.3906\n",
      "Epoch 14/20\n",
      "496/483 [==============================] - 207s - loss: 1.4080 - acc: 0.5040 - val_loss: 1.6191 - val_acc: 0.4609\n",
      "Epoch 15/20\n",
      "496/483 [==============================] - 211s - loss: 1.2431 - acc: 0.5504 - val_loss: 1.7571 - val_acc: 0.3516\n",
      "Epoch 16/20\n",
      "496/483 [==============================] - 218s - loss: 1.3893 - acc: 0.5060 - val_loss: 1.5152 - val_acc: 0.4688\n",
      "Epoch 17/20\n",
      "496/483 [==============================] - 216s - loss: 1.3060 - acc: 0.5242 - val_loss: 1.4650 - val_acc: 0.5156\n",
      "Epoch 18/20\n",
      "496/483 [==============================] - 207s - loss: 1.3113 - acc: 0.5565 - val_loss: 1.3846 - val_acc: 0.4688\n",
      "Epoch 19/20\n",
      "496/483 [==============================] - 206s - loss: 1.2530 - acc: 0.5504 - val_loss: 1.5454 - val_acc: 0.4453\n",
      "Epoch 20/20\n",
      "496/483 [==============================] - 206s - loss: 1.3532 - acc: 0.5161 - val_loss: 1.4214 - val_acc: 0.4922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13adf9cd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=(nb_train_samples // batch_size),\n",
    "    nb_epoch=20,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=(nb_validation_samples // batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_path = '/Users/Josevi/product_images/photo/9760-1.jpg'\n",
    "img = load_img(img_path, target_size=(224, 224))\n",
    "x = img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(mpimg.imread(img_path))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "preds = model.predict(x)\n",
    "print preds\n",
    "#print('Predicted:', decode_predictions(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_sample(X, y, prediction=-1):\n",
    "    im = X\n",
    "    print y\n",
    "    #y = np.flip(y, axis=0)\n",
    "    y_label = class_name[np.nonzero(y)[0][0]]\n",
    "    plt.imshow(im)\n",
    "    if prediction >= 0:\n",
    "        plt.title(\"Class = %s, Predict = %s\" % (y_label, class_name[prediction]))\n",
    "    else:\n",
    "        plt.title(\"Class = %s\" % (y_label))\n",
    "\n",
    "    plt.axis('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for X_batch, Y_batch in train_generator:\n",
    "    for i in range(len(Y_batch)):\n",
    "        show_sample(X_batch[i, :, :, :], Y_batch[i])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 - DLforCV",
   "language": "python",
   "name": "python2-dlforcv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
