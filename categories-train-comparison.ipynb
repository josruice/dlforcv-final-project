{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frictionless Product Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author:** Jose Vicente Ruiz Cepeda (jr3660)\n",
    "- **Course:** COMS W 4995 - Deep Learning for Computer Vision\n",
    "- **Assignment**: Final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain some of the context of the problem here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras # Keras 1.2.2 assumed.\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications import Xception\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Required to avoid errors with the images.\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = {\n",
    "    0: 'photo',\n",
    "    1: 'electronics',\n",
    "    2: 'events',\n",
    "    3: 'instruments',\n",
    "    4: 'tools',\n",
    "    5: 'sports',\n",
    "    6: 'caravans',\n",
    "    7: 'others'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I - Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = './data/train'\n",
    "validation_data_dir = './data/validation'\n",
    "test_data_dir = './data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Categorizer:\n",
    "    def __init__(self, num_classes, num_dense_layer_units=256, dropout=0.5):\n",
    "        self.num_classes = num_classes\n",
    "        self.num_dense_layer_units = num_dense_layer_units\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def build_model(self, ImagenetModel=ResNet50, verbose=False):\n",
    "        # Keras models are functions, not classes, so we have to check which one is it like this.\n",
    "        if ImagenetModel.func_name == 'InceptionV3' or ImagenetModel.func_name == 'Xception':\n",
    "            self.img_width, self.img_height = 299, 299\n",
    "        else:\n",
    "            self.img_width, self.img_height = 224, 224\n",
    "\n",
    "        # First, let's load the model with ImageNet weights and without the top layer.\n",
    "        # This will take some time the first time, since the weights have to be downloaded.\n",
    "        model = ImagenetModel(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(self.img_width, self.img_height, 3)\n",
    "        )\n",
    "\n",
    "        #Â Now, let's create the top layers adapted to our problem. For that we use the\n",
    "        # Functional API of Keras. (https://keras.io/getting-started/functional-api-guide/)\n",
    "        x = model.output\n",
    "        x = Flatten(input_shape=model.output_shape[1:])(x)\n",
    "        x = Dense(self.num_dense_layer_units, activation='relu')(x)\n",
    "        x = Dropout(self.dropout)(x)\n",
    "        preds = Dense(self.num_classes, activation='softmax')(x)\n",
    "        self.num_top_layers = 4\n",
    "\n",
    "        # Combine both models to get the final one.\n",
    "        self.model = Model(model.input, preds)\n",
    "\n",
    "        if verbose:\n",
    "            self.model.summary()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def compile_model(self, optimizer=optimizers.Adagrad(lr=0.001)):\n",
    "        self.is_compiled = True\n",
    "        self.optimizer = optimizer\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def _recompile_model(self):\n",
    "        if self.is_compiled != True:\n",
    "            raise Error(\"Model has to be compiled first.\")\n",
    "\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=self.optimizer,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    def fine_tune(self,\n",
    "                  class_names,\n",
    "                  train_data_dir,\n",
    "                  validation_data_dir,\n",
    "                  batch_size=16,\n",
    "                  num_only_top_epochs=10,\n",
    "                  num_whole_model_epochs=40,\n",
    "                  best_model_path=None,\n",
    "                  tensorboard_logs_path=None,\n",
    "                  reduce_learning_rate=True):\n",
    "        \n",
    "        # Augmentation configurations for training and validation.\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "        validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            train_data_dir,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=batch_size,\n",
    "            classes=class_names,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        validation_generator = validation_datagen.flow_from_directory(\n",
    "            validation_data_dir,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=batch_size,\n",
    "            classes=class_names,\n",
    "            class_mode='categorical')\n",
    "        \n",
    "        if num_only_top_epochs > 0:\n",
    "            # Freeze everything except the top layers, before training.\n",
    "            for layer in self.model.layers[:-self.num_top_layers]:\n",
    "                layer.trainable = False \n",
    "\n",
    "            print \"Starting with top layers training...\"\n",
    "            self._fit_generator(\n",
    "                train_generator,\n",
    "                validation_generator,\n",
    "                batch_size,\n",
    "                num_only_top_epochs,\n",
    "                best_model_path=best_model_path,\n",
    "                tensorboard_logs_path=tensorboard_logs_path,\n",
    "                tensorboard_logs_path_suffix='Top',\n",
    "                reduce_learning_rate=True)\n",
    "            print \"Top layers training done.\"\n",
    "            \n",
    "        if num_whole_model_epochs > 0:\n",
    "            # Unfreeze everything and train for some more epochs.\n",
    "            for layer in self.model.layers:\n",
    "                layer.trainable = True\n",
    "            \n",
    "            print \"Starting with whole model training...\"\n",
    "            self._fit_generator(\n",
    "                train_generator,\n",
    "                validation_generator,\n",
    "                batch_size,\n",
    "                num_whole_model_epochs,\n",
    "                best_model_path=best_model_path,\n",
    "                tensorboard_logs_path=tensorboard_logs_path,\n",
    "                tensorboard_logs_path_suffix='Whole',\n",
    "                reduce_learning_rate=True)\n",
    "            print \"Whole model training done\"\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _fit_generator(self,\n",
    "                       train_generator,\n",
    "                       validation_generator,\n",
    "                       batch_size,\n",
    "                       num_epochs,\n",
    "                       best_model_path=None,\n",
    "                       tensorboard_logs_path=None,\n",
    "                       tensorboard_logs_path_suffix='',\n",
    "                       reduce_learning_rate=True):\n",
    "        # Recompile model before training to increase efficiency in\n",
    "        # case of frozen layers.\n",
    "        self._recompile_model()\n",
    "        \n",
    "        # Save the best model based on validation accuracy.\n",
    "        if best_model_path:\n",
    "            model_checkpoint = ModelCheckpoint(\n",
    "                best_model_path,\n",
    "                monitor='val_acc',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                period=1\n",
    "            )\n",
    "        \n",
    "        #Â Log epochs information like loss and accuracy to review it\n",
    "        # afterwards using TensorBoard.\n",
    "        if tensorboard_logs_path:\n",
    "            if tensorboard_logs_path[-1] == '/':\n",
    "                tensorboard_logs_path = tensorboard_logs_path[:-1]\n",
    "            tensorboard = TensorBoard(\n",
    "                log_dir=tensorboard_logs_path+'-'+tensorboard_logs_path_suffix+'/')\n",
    "        \n",
    "        # Reduce learning rate i\n",
    "        if reduce_learning_rate:\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "        \n",
    "        self.model.fit_generator(\n",
    "            train_generator,\n",
    "            samples_per_epoch=train_generator.n//batch_size,\n",
    "            nb_epoch=num_epochs,\n",
    "            validation_data=validation_generator,\n",
    "            nb_val_samples=validation_generator.n//batch_size,\n",
    "            callbacks=[model_checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VGG16\n",
      "Found 13929 images belonging to 8 classes.\n",
      "Found 3489 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/20\n",
      "864/870 [============================>.] - ETA: 0s - loss: 2.3606 - acc: 0.2998"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/870 [==============================] - 22s - loss: 2.3455 - acc: 0.3034 - val_loss: 1.4900 - val_acc: 0.4911\n",
      "Epoch 2/20\n",
      "880/870 [==============================] - 20s - loss: 1.6454 - acc: 0.3761 - val_loss: 1.4730 - val_acc: 0.4643\n",
      "Epoch 3/20\n",
      "880/870 [==============================] - 20s - loss: 1.5873 - acc: 0.4136 - val_loss: 1.5075 - val_acc: 0.4464\n",
      "Epoch 4/20\n",
      "864/870 [============================>.] - ETA: 0s - loss: 1.5814 - acc: 0.3970"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/PIL/Image.py:857: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880/870 [==============================] - 21s - loss: 1.5808 - acc: 0.3955 - val_loss: 1.4112 - val_acc: 0.5089\n",
      "Epoch 5/20\n",
      "880/870 [==============================] - 20s - loss: 1.5205 - acc: 0.4557 - val_loss: 1.3608 - val_acc: 0.4955\n",
      "Epoch 6/20\n",
      "880/870 [==============================] - 20s - loss: 1.4816 - acc: 0.4557 - val_loss: 1.4304 - val_acc: 0.4777\n",
      "Epoch 7/20\n",
      "880/870 [==============================] - 21s - loss: 1.5073 - acc: 0.4432 - val_loss: 1.3421 - val_acc: 0.5536\n",
      "Epoch 8/20\n",
      "880/870 [==============================] - 20s - loss: 1.5197 - acc: 0.4398 - val_loss: 1.3651 - val_acc: 0.4821\n",
      "Epoch 9/20\n",
      "880/870 [==============================] - 20s - loss: 1.5023 - acc: 0.4580 - val_loss: 1.3989 - val_acc: 0.4732\n",
      "Epoch 10/20\n",
      "880/870 [==============================] - 21s - loss: 1.4493 - acc: 0.4625 - val_loss: 1.3271 - val_acc: 0.5022\n",
      "Epoch 11/20\n",
      "880/870 [==============================] - 20s - loss: 1.3771 - acc: 0.4864 - val_loss: 1.3243 - val_acc: 0.5179\n",
      "Epoch 12/20\n",
      "880/870 [==============================] - 21s - loss: 1.4065 - acc: 0.4807 - val_loss: 1.2876 - val_acc: 0.5804\n",
      "Epoch 13/20\n",
      "880/870 [==============================] - 20s - loss: 1.4081 - acc: 0.4830 - val_loss: 1.2239 - val_acc: 0.5491\n",
      "Epoch 14/20\n",
      "880/870 [==============================] - 20s - loss: 1.3883 - acc: 0.5148 - val_loss: 1.2412 - val_acc: 0.5714\n",
      "Epoch 15/20\n",
      "880/870 [==============================] - 20s - loss: 1.4160 - acc: 0.4739 - val_loss: 1.2788 - val_acc: 0.5536\n",
      "Epoch 16/20\n",
      "873/870 [==============================] - 22s - loss: 1.3997 - acc: 0.4845 - val_loss: 1.2377 - val_acc: 0.6027\n",
      "Epoch 17/20\n",
      "880/870 [==============================] - 20s - loss: 1.2819 - acc: 0.5375 - val_loss: 1.1729 - val_acc: 0.5759\n",
      "Epoch 18/20\n",
      "880/870 [==============================] - 22s - loss: 1.3487 - acc: 0.5114 - val_loss: 1.1974 - val_acc: 0.6027\n",
      "Epoch 19/20\n",
      "880/870 [==============================] - 22s - loss: 1.3044 - acc: 0.5330 - val_loss: 1.2287 - val_acc: 0.5714\n",
      "Epoch 20/20\n",
      "880/870 [==============================] - 21s - loss: 1.3288 - acc: 0.5432 - val_loss: 1.1993 - val_acc: 0.5714\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/80\n",
      "880/870 [==============================] - 58s - loss: 11.9118 - acc: 0.2523 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 2/80\n",
      "880/870 [==============================] - 55s - loss: 12.4183 - acc: 0.2295 - val_loss: 12.8801 - val_acc: 0.2009\n",
      "Epoch 3/80\n",
      "880/870 [==============================] - 55s - loss: 12.7663 - acc: 0.2080 - val_loss: 12.2325 - val_acc: 0.2411\n",
      "Epoch 4/80\n",
      "880/870 [==============================] - 55s - loss: 13.0410 - acc: 0.1909 - val_loss: 11.9447 - val_acc: 0.2589\n",
      "Epoch 5/80\n",
      "880/870 [==============================] - 54s - loss: 12.7663 - acc: 0.2080 - val_loss: 12.4484 - val_acc: 0.2277\n",
      "Epoch 6/80\n",
      "880/870 [==============================] - 54s - loss: 12.4183 - acc: 0.2295 - val_loss: 12.8801 - val_acc: 0.2009\n",
      "Epoch 7/80\n",
      "880/870 [==============================] - 55s - loss: 12.7113 - acc: 0.2114 - val_loss: 11.7288 - val_acc: 0.2723\n",
      "Epoch 8/80\n",
      "880/870 [==============================] - 54s - loss: 12.7663 - acc: 0.2080 - val_loss: 12.3044 - val_acc: 0.2366\n",
      "Epoch 9/80\n",
      "880/870 [==============================] - 54s - loss: 12.5098 - acc: 0.2239 - val_loss: 12.8945 - val_acc: 0.2000\n",
      "Epoch 10/80\n",
      "880/870 [==============================] - 54s - loss: 12.7846 - acc: 0.2068 - val_loss: 12.2325 - val_acc: 0.2411\n",
      "Epoch 11/80\n",
      "880/870 [==============================] - 54s - loss: 12.5465 - acc: 0.2216 - val_loss: 13.1679 - val_acc: 0.1830\n",
      "Epoch 12/80\n",
      "873/870 [==============================] - 55s - loss: 12.3701 - acc: 0.2325 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 13/80\n",
      "880/870 [==============================] - 54s - loss: 12.7479 - acc: 0.2091 - val_loss: 13.1679 - val_acc: 0.1830\n",
      "Epoch 14/80\n",
      "880/870 [==============================] - 54s - loss: 12.5831 - acc: 0.2193 - val_loss: 12.5203 - val_acc: 0.2232\n",
      "Epoch 15/80\n",
      "880/870 [==============================] - 54s - loss: 12.6197 - acc: 0.2170 - val_loss: 12.6642 - val_acc: 0.2143\n",
      "Epoch 16/80\n",
      "880/870 [==============================] - 54s - loss: 12.5465 - acc: 0.2216 - val_loss: 12.7362 - val_acc: 0.2098\n",
      "Epoch 17/80\n",
      "880/870 [==============================] - 55s - loss: 12.3816 - acc: 0.2318 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 18/80\n",
      "880/870 [==============================] - 54s - loss: 12.5465 - acc: 0.2216 - val_loss: 13.5392 - val_acc: 0.1600\n",
      "Epoch 19/80\n",
      "880/870 [==============================] - 54s - loss: 12.2717 - acc: 0.2386 - val_loss: 12.7362 - val_acc: 0.2098\n",
      "Epoch 20/80\n",
      "880/870 [==============================] - 54s - loss: 12.9311 - acc: 0.1977 - val_loss: 12.3044 - val_acc: 0.2366\n",
      "Epoch 21/80\n",
      "880/870 [==============================] - 54s - loss: 12.5831 - acc: 0.2193 - val_loss: 12.7362 - val_acc: 0.2098\n",
      "Epoch 22/80\n",
      "880/870 [==============================] - 54s - loss: 12.6747 - acc: 0.2136 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 23/80\n",
      "880/870 [==============================] - 54s - loss: 12.6747 - acc: 0.2136 - val_loss: 12.7362 - val_acc: 0.2098\n",
      "Epoch 24/80\n",
      "880/870 [==============================] - 54s - loss: 12.4549 - acc: 0.2273 - val_loss: 12.8801 - val_acc: 0.2009\n",
      "Epoch 25/80\n",
      "880/870 [==============================] - 54s - loss: 12.7479 - acc: 0.2091 - val_loss: 12.4484 - val_acc: 0.2277\n",
      "Epoch 26/80\n",
      "880/870 [==============================] - 54s - loss: 12.6564 - acc: 0.2148 - val_loss: 12.0166 - val_acc: 0.2545\n",
      "Epoch 27/80\n",
      "880/870 [==============================] - 54s - loss: 12.4183 - acc: 0.2295 - val_loss: 12.1781 - val_acc: 0.2444\n",
      "Epoch 28/80\n",
      "873/870 [==============================] - 54s - loss: 12.4071 - acc: 0.2302 - val_loss: 12.5203 - val_acc: 0.2232\n",
      "Epoch 29/80\n",
      "880/870 [==============================] - 54s - loss: 12.6014 - acc: 0.2182 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 30/80\n",
      "880/870 [==============================] - 54s - loss: 12.5648 - acc: 0.2205 - val_loss: 12.3764 - val_acc: 0.2321\n",
      "Epoch 31/80\n",
      "880/870 [==============================] - 54s - loss: 12.7296 - acc: 0.2102 - val_loss: 13.0240 - val_acc: 0.1920\n",
      "Epoch 32/80\n",
      "880/870 [==============================] - 54s - loss: 12.6747 - acc: 0.2136 - val_loss: 12.6642 - val_acc: 0.2143\n",
      "Epoch 33/80\n",
      "880/870 [==============================] - 54s - loss: 12.6197 - acc: 0.2170 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 34/80\n",
      "880/870 [==============================] - 55s - loss: 12.5098 - acc: 0.2239 - val_loss: 12.6642 - val_acc: 0.2143\n",
      "Epoch 35/80\n",
      "880/870 [==============================] - 54s - loss: 12.8212 - acc: 0.2045 - val_loss: 13.0960 - val_acc: 0.1875\n",
      "Epoch 36/80\n",
      "880/870 [==============================] - 54s - loss: 12.9677 - acc: 0.1955 - val_loss: 12.4484 - val_acc: 0.2277\n",
      "Epoch 37/80\n",
      "880/870 [==============================] - 54s - loss: 12.2717 - acc: 0.2386 - val_loss: 13.0960 - val_acc: 0.1875\n",
      "Epoch 38/80\n",
      "880/870 [==============================] - 54s - loss: 12.4366 - acc: 0.2284 - val_loss: 12.3764 - val_acc: 0.2321\n",
      "Epoch 39/80\n",
      "880/870 [==============================] - 54s - loss: 12.4915 - acc: 0.2250 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 40/80\n",
      "880/870 [==============================] - 54s - loss: 12.3816 - acc: 0.2318 - val_loss: 12.0166 - val_acc: 0.2545\n",
      "Epoch 41/80\n",
      "880/870 [==============================] - 54s - loss: 12.4366 - acc: 0.2284 - val_loss: 13.0240 - val_acc: 0.1920\n",
      "Epoch 42/80\n",
      "880/870 [==============================] - 54s - loss: 12.7296 - acc: 0.2102 - val_loss: 12.6642 - val_acc: 0.2143\n",
      "Epoch 43/80\n",
      "880/870 [==============================] - 54s - loss: 12.6197 - acc: 0.2170 - val_loss: 12.1605 - val_acc: 0.2455\n",
      "Epoch 44/80\n",
      "873/870 [==============================] - 54s - loss: 12.6101 - acc: 0.2176 - val_loss: 11.8727 - val_acc: 0.2634\n",
      "Epoch 45/80\n",
      "880/870 [==============================] - 54s - loss: 12.5648 - acc: 0.2205 - val_loss: 12.5923 - val_acc: 0.2188\n",
      "Epoch 46/80\n",
      "880/870 [==============================] - 54s - loss: 12.8029 - acc: 0.2057 - val_loss: 12.8228 - val_acc: 0.2044\n",
      "Epoch 47/80\n",
      "880/870 [==============================] - 54s - loss: 12.2351 - acc: 0.2409 - val_loss: 11.8007 - val_acc: 0.2679\n",
      "Epoch 48/80\n",
      "880/870 [==============================] - 54s - loss: 12.8762 - acc: 0.2011 - val_loss: 12.2325 - val_acc: 0.2411\n",
      "Epoch 49/80\n",
      "880/870 [==============================] - 54s - loss: 12.3816 - acc: 0.2318 - val_loss: 13.0960 - val_acc: 0.1875\n",
      "Epoch 50/80\n",
      "880/870 [==============================] - 54s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.5923 - val_acc: 0.2188\n",
      "Epoch 51/80\n",
      "880/870 [==============================] - 54s - loss: 13.0044 - acc: 0.1932 - val_loss: 12.4484 - val_acc: 0.2277\n",
      "Epoch 52/80\n",
      "880/870 [==============================] - 54s - loss: 12.5831 - acc: 0.2193 - val_loss: 12.3764 - val_acc: 0.2321\n",
      "Epoch 53/80\n",
      "880/870 [==============================] - 54s - loss: 12.3633 - acc: 0.2330 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 54/80\n",
      "880/870 [==============================] - 54s - loss: 12.5282 - acc: 0.2227 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 55/80\n",
      "880/870 [==============================] - 55s - loss: 12.7296 - acc: 0.2102 - val_loss: 11.2468 - val_acc: 0.3022\n",
      "Epoch 56/80\n",
      "880/870 [==============================] - 54s - loss: 12.9494 - acc: 0.1966 - val_loss: 12.5923 - val_acc: 0.2188\n",
      "Epoch 57/80\n",
      "880/870 [==============================] - 54s - loss: 12.2900 - acc: 0.2375 - val_loss: 12.3764 - val_acc: 0.2321\n",
      "Epoch 58/80\n",
      "880/870 [==============================] - 54s - loss: 12.5282 - acc: 0.2227 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 59/80\n",
      "873/870 [==============================] - 53s - loss: 12.8502 - acc: 0.2027 - val_loss: 12.4484 - val_acc: 0.2277\n",
      "Epoch 60/80\n",
      "880/870 [==============================] - 54s - loss: 12.7846 - acc: 0.2068 - val_loss: 12.6642 - val_acc: 0.2143\n",
      "Epoch 61/80\n",
      "880/870 [==============================] - 54s - loss: 12.8578 - acc: 0.2023 - val_loss: 12.6642 - val_acc: 0.2143\n",
      "Epoch 62/80\n",
      "880/870 [==============================] - 54s - loss: 12.5098 - acc: 0.2239 - val_loss: 12.3764 - val_acc: 0.2321\n",
      "Epoch 63/80\n",
      "880/870 [==============================] - 54s - loss: 12.4915 - acc: 0.2250 - val_loss: 12.6642 - val_acc: 0.2143\n",
      "Epoch 64/80\n",
      "880/870 [==============================] - 54s - loss: 12.5282 - acc: 0.2227 - val_loss: 11.9632 - val_acc: 0.2578\n",
      "Epoch 65/80\n",
      "880/870 [==============================] - 54s - loss: 12.6930 - acc: 0.2125 - val_loss: 12.5923 - val_acc: 0.2188\n",
      "Epoch 66/80\n",
      "880/870 [==============================] - 54s - loss: 12.6197 - acc: 0.2170 - val_loss: 12.3764 - val_acc: 0.2321\n",
      "Epoch 67/80\n",
      "880/870 [==============================] - 54s - loss: 12.2717 - acc: 0.2386 - val_loss: 12.5923 - val_acc: 0.2188\n",
      "Epoch 68/80\n",
      "880/870 [==============================] - 54s - loss: 12.8762 - acc: 0.2011 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 69/80\n",
      "880/870 [==============================] - 55s - loss: 12.4915 - acc: 0.2250 - val_loss: 12.8081 - val_acc: 0.2054\n",
      "Epoch 70/80\n",
      "880/870 [==============================] - 54s - loss: 12.5831 - acc: 0.2193 - val_loss: 12.8801 - val_acc: 0.2009\n",
      "Epoch 71/80\n",
      "880/870 [==============================] - 54s - loss: 12.7846 - acc: 0.2068 - val_loss: 13.0960 - val_acc: 0.1875\n",
      "Epoch 72/80\n",
      "880/870 [==============================] - 54s - loss: 12.5098 - acc: 0.2239 - val_loss: 11.3690 - val_acc: 0.2946\n",
      "Epoch 73/80\n",
      "880/870 [==============================] - 54s - loss: 12.5098 - acc: 0.2239 - val_loss: 12.5203 - val_acc: 0.2232\n",
      "Epoch 74/80\n",
      "880/870 [==============================] - 54s - loss: 12.2717 - acc: 0.2386 - val_loss: 13.0240 - val_acc: 0.1920\n",
      "Epoch 75/80\n",
      "873/870 [==============================] - 54s - loss: 12.5917 - acc: 0.2188 - val_loss: 12.2325 - val_acc: 0.2411\n",
      "Epoch 76/80\n",
      "880/870 [==============================] - 54s - loss: 12.5648 - acc: 0.2205 - val_loss: 12.3764 - val_acc: 0.2321\n",
      "Epoch 77/80\n",
      "880/870 [==============================] - 54s - loss: 12.7296 - acc: 0.2102 - val_loss: 12.3764 - val_acc: 0.2321\n",
      "Epoch 78/80\n",
      "880/870 [==============================] - 54s - loss: 12.3633 - acc: 0.2330 - val_loss: 11.4410 - val_acc: 0.2902\n",
      "Epoch 79/80\n",
      "880/870 [==============================] - 54s - loss: 12.6381 - acc: 0.2159 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 80/80\n",
      "880/870 [==============================] - 54s - loss: 12.7663 - acc: 0.2080 - val_loss: 13.3838 - val_acc: 0.1696\n",
      "Whole model training done\n"
     ]
    }
   ],
   "source": [
    "print \"Training VGG16\"\n",
    "categorizer = Categorizer(len(CLASS_NAMES)). \\\n",
    "    build_model(VGG16). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        CLASS_NAMES.values(),\n",
    "        train_data_dir,\n",
    "        validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/VGG16.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG16'\n",
    "    )\n",
    "del categorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VGG19\n",
      "Found 13929 images belonging to 8 classes.\n",
      "Found 3489 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/20\n",
      "880/870 [==============================] - 26s - loss: 2.4984 - acc: 0.3080 - val_loss: 1.6231 - val_acc: 0.3884\n",
      "Epoch 2/20\n",
      "880/870 [==============================] - 26s - loss: 1.7209 - acc: 0.3648 - val_loss: 1.5196 - val_acc: 0.4598\n",
      "Epoch 3/20\n",
      "880/870 [==============================] - 26s - loss: 1.7002 - acc: 0.3523 - val_loss: 1.5050 - val_acc: 0.4643\n",
      "Epoch 4/20\n",
      "880/870 [==============================] - 25s - loss: 1.5933 - acc: 0.3875 - val_loss: 1.4419 - val_acc: 0.4018\n",
      "Epoch 5/20\n",
      "880/870 [==============================] - 26s - loss: 1.6020 - acc: 0.4216 - val_loss: 1.4453 - val_acc: 0.4821\n",
      "Epoch 6/20\n",
      "880/870 [==============================] - 25s - loss: 1.5567 - acc: 0.4318 - val_loss: 1.4306 - val_acc: 0.4688\n",
      "Epoch 7/20\n",
      "880/870 [==============================] - 26s - loss: 1.5280 - acc: 0.4455 - val_loss: 1.3448 - val_acc: 0.5089\n",
      "Epoch 8/20\n",
      "880/870 [==============================] - 25s - loss: 1.4944 - acc: 0.4750 - val_loss: 1.3644 - val_acc: 0.5089\n",
      "Epoch 9/20\n",
      "880/870 [==============================] - 25s - loss: 1.5187 - acc: 0.4386 - val_loss: 1.3953 - val_acc: 0.4866\n",
      "Epoch 10/20\n",
      "880/870 [==============================] - 25s - loss: 1.5091 - acc: 0.4409 - val_loss: 1.3401 - val_acc: 0.5067\n",
      "Epoch 11/20\n",
      "880/870 [==============================] - 26s - loss: 1.4710 - acc: 0.4591 - val_loss: 1.3196 - val_acc: 0.5134\n",
      "Epoch 12/20\n",
      "880/870 [==============================] - 26s - loss: 1.4518 - acc: 0.4716 - val_loss: 1.3147 - val_acc: 0.5179\n",
      "Epoch 13/20\n",
      "880/870 [==============================] - 26s - loss: 1.4749 - acc: 0.4375 - val_loss: 1.2725 - val_acc: 0.5312\n",
      "Epoch 14/20\n",
      "880/870 [==============================] - 27s - loss: 1.4462 - acc: 0.4830 - val_loss: 1.3003 - val_acc: 0.5402\n",
      "Epoch 15/20\n",
      "880/870 [==============================] - 25s - loss: 1.4139 - acc: 0.4818 - val_loss: 1.2575 - val_acc: 0.5402\n",
      "Epoch 16/20\n",
      "873/870 [==============================] - 26s - loss: 1.4066 - acc: 0.4834 - val_loss: 1.2816 - val_acc: 0.5536\n",
      "Epoch 17/20\n",
      "880/870 [==============================] - 25s - loss: 1.4332 - acc: 0.4716 - val_loss: 1.2721 - val_acc: 0.5268\n",
      "Epoch 18/20\n",
      "880/870 [==============================] - 25s - loss: 1.3927 - acc: 0.5080 - val_loss: 1.3870 - val_acc: 0.4777\n",
      "Epoch 19/20\n",
      "880/870 [==============================] - 26s - loss: 1.3981 - acc: 0.4989 - val_loss: 1.3010 - val_acc: 0.5580\n",
      "Epoch 20/20\n",
      "880/870 [==============================] - 25s - loss: 1.3749 - acc: 0.4943 - val_loss: 1.2726 - val_acc: 0.5536\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/80\n",
      "880/870 [==============================] - 68s - loss: 13.9075 - acc: 0.1284 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 2/80\n",
      "880/870 [==============================] - 66s - loss: 13.7187 - acc: 0.1489 - val_loss: 14.3912 - val_acc: 0.1071\n",
      "Epoch 3/80\n",
      "880/870 [==============================] - 68s - loss: 13.9019 - acc: 0.1375 - val_loss: 13.6716 - val_acc: 0.1518\n",
      "Epoch 4/80\n",
      "880/870 [==============================] - 66s - loss: 14.1766 - acc: 0.1205 - val_loss: 14.0314 - val_acc: 0.1295\n",
      "Epoch 5/80\n",
      "880/870 [==============================] - 66s - loss: 13.6271 - acc: 0.1545 - val_loss: 13.8875 - val_acc: 0.1384\n",
      "Epoch 6/80\n",
      "880/870 [==============================] - 66s - loss: 13.8835 - acc: 0.1386 - val_loss: 14.3192 - val_acc: 0.1116\n",
      "Epoch 7/80\n",
      "880/870 [==============================] - 66s - loss: 14.0484 - acc: 0.1284 - val_loss: 13.9594 - val_acc: 0.1339\n",
      "Epoch 8/80\n",
      "880/870 [==============================] - 68s - loss: 14.2132 - acc: 0.1182 - val_loss: 13.5277 - val_acc: 0.1607\n",
      "Epoch 9/80\n",
      "880/870 [==============================] - 66s - loss: 13.8652 - acc: 0.1398 - val_loss: 13.7436 - val_acc: 0.1473\n",
      "Epoch 10/80\n",
      "880/870 [==============================] - 68s - loss: 13.9385 - acc: 0.1352 - val_loss: 13.3118 - val_acc: 0.1741\n",
      "Epoch 11/80\n",
      "880/870 [==============================] - 66s - loss: 13.9385 - acc: 0.1352 - val_loss: 14.1753 - val_acc: 0.1205\n",
      "Epoch 12/80\n",
      "873/870 [==============================] - 66s - loss: 13.7733 - acc: 0.1455 - val_loss: 14.3192 - val_acc: 0.1116\n",
      "Epoch 13/80\n",
      "880/870 [==============================] - 66s - loss: 13.7553 - acc: 0.1466 - val_loss: 13.9594 - val_acc: 0.1339\n",
      "Epoch 14/80\n",
      "880/870 [==============================] - 66s - loss: 13.9385 - acc: 0.1352 - val_loss: 13.6716 - val_acc: 0.1518\n",
      "Epoch 15/80\n",
      "880/870 [==============================] - 66s - loss: 13.4073 - acc: 0.1682 - val_loss: 14.0314 - val_acc: 0.1295\n",
      "Epoch 16/80\n",
      "880/870 [==============================] - 68s - loss: 13.7553 - acc: 0.1466 - val_loss: 13.1679 - val_acc: 0.1830\n",
      "Epoch 17/80\n",
      "880/870 [==============================] - 66s - loss: 13.9202 - acc: 0.1364 - val_loss: 14.3192 - val_acc: 0.1116\n",
      "Epoch 18/80\n",
      "880/870 [==============================] - 66s - loss: 13.9568 - acc: 0.1341 - val_loss: 13.6825 - val_acc: 0.1511\n",
      "Epoch 19/80\n",
      "880/870 [==============================] - 66s - loss: 13.9934 - acc: 0.1318 - val_loss: 14.1753 - val_acc: 0.1205\n",
      "Epoch 20/80\n",
      "880/870 [==============================] - 66s - loss: 13.7920 - acc: 0.1443 - val_loss: 13.6716 - val_acc: 0.1518\n",
      "Epoch 21/80\n",
      "880/870 [==============================] - 66s - loss: 13.7004 - acc: 0.1500 - val_loss: 13.3838 - val_acc: 0.1696\n",
      "Epoch 22/80\n",
      "880/870 [==============================] - 66s - loss: 14.1033 - acc: 0.1250 - val_loss: 13.2399 - val_acc: 0.1786\n",
      "Epoch 23/80\n",
      "880/870 [==============================] - 66s - loss: 13.9019 - acc: 0.1375 - val_loss: 13.6716 - val_acc: 0.1518\n",
      "Epoch 24/80\n",
      "880/870 [==============================] - 66s - loss: 14.1766 - acc: 0.1205 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 25/80\n",
      "880/870 [==============================] - 66s - loss: 13.9019 - acc: 0.1375 - val_loss: 14.5351 - val_acc: 0.0982\n",
      "Epoch 26/80\n",
      "880/870 [==============================] - 66s - loss: 14.1216 - acc: 0.1239 - val_loss: 13.5277 - val_acc: 0.1607\n",
      "Epoch 27/80\n",
      "880/870 [==============================] - 66s - loss: 13.9568 - acc: 0.1341 - val_loss: 13.7541 - val_acc: 0.1467\n",
      "Epoch 28/80\n",
      "873/870 [==============================] - 66s - loss: 13.9764 - acc: 0.1329 - val_loss: 13.9594 - val_acc: 0.1339\n",
      "Epoch 29/80\n",
      "880/870 [==============================] - 66s - loss: 13.9751 - acc: 0.1330 - val_loss: 13.3118 - val_acc: 0.1741\n",
      "Epoch 30/80\n",
      "880/870 [==============================] - 66s - loss: 13.9385 - acc: 0.1352 - val_loss: 13.9594 - val_acc: 0.1339\n",
      "Epoch 31/80\n",
      "880/870 [==============================] - 66s - loss: 13.7920 - acc: 0.1443 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 32/80\n",
      "880/870 [==============================] - 66s - loss: 13.5722 - acc: 0.1580 - val_loss: 13.9594 - val_acc: 0.1339\n",
      "Epoch 33/80\n",
      "880/870 [==============================] - 66s - loss: 13.9385 - acc: 0.1352 - val_loss: 14.1033 - val_acc: 0.1250\n",
      "Epoch 34/80\n",
      "880/870 [==============================] - 66s - loss: 13.9385 - acc: 0.1352 - val_loss: 14.0314 - val_acc: 0.1295\n",
      "Epoch 35/80\n",
      "880/870 [==============================] - 66s - loss: 14.3231 - acc: 0.1114 - val_loss: 13.8155 - val_acc: 0.1429\n",
      "Epoch 36/80\n",
      "880/870 [==============================] - 66s - loss: 13.8469 - acc: 0.1409 - val_loss: 14.3272 - val_acc: 0.1111\n",
      "Epoch 37/80\n",
      "880/870 [==============================] - 66s - loss: 13.5539 - acc: 0.1591 - val_loss: 13.9594 - val_acc: 0.1339\n",
      "Epoch 38/80\n",
      "880/870 [==============================] - 66s - loss: 13.8103 - acc: 0.1432 - val_loss: 13.8875 - val_acc: 0.1384\n",
      "Epoch 39/80\n",
      "880/870 [==============================] - 66s - loss: 13.8835 - acc: 0.1386 - val_loss: 14.1033 - val_acc: 0.1250\n",
      "Epoch 40/80\n",
      "880/870 [==============================] - 66s - loss: 13.9568 - acc: 0.1341 - val_loss: 13.8155 - val_acc: 0.1429\n",
      "Epoch 41/80\n",
      "880/870 [==============================] - 68s - loss: 13.8103 - acc: 0.1432 - val_loss: 13.0960 - val_acc: 0.1875\n",
      "Epoch 42/80\n",
      "880/870 [==============================] - 66s - loss: 14.1216 - acc: 0.1239 - val_loss: 14.3192 - val_acc: 0.1116\n",
      "Epoch 43/80\n",
      "880/870 [==============================] - 66s - loss: 13.8652 - acc: 0.1398 - val_loss: 14.0314 - val_acc: 0.1295\n",
      "Epoch 44/80\n",
      "873/870 [==============================] - 66s - loss: 14.3826 - acc: 0.1077 - val_loss: 14.3192 - val_acc: 0.1116\n",
      "Epoch 45/80\n",
      "880/870 [==============================] - 66s - loss: 13.7736 - acc: 0.1455 - val_loss: 13.6825 - val_acc: 0.1511\n",
      "Epoch 46/80\n",
      "880/870 [==============================] - 66s - loss: 13.9202 - acc: 0.1364 - val_loss: 13.6716 - val_acc: 0.1518\n",
      "Epoch 47/80\n",
      "880/870 [==============================] - 66s - loss: 13.9934 - acc: 0.1318 - val_loss: 14.1033 - val_acc: 0.1250\n",
      "Epoch 48/80\n",
      "880/870 [==============================] - 66s - loss: 14.0301 - acc: 0.1295 - val_loss: 13.6716 - val_acc: 0.1518\n",
      "Epoch 49/80\n",
      "880/870 [==============================] - 66s - loss: 13.8103 - acc: 0.1432 - val_loss: 14.1033 - val_acc: 0.1250\n",
      "Epoch 50/80\n",
      "880/870 [==============================] - 66s - loss: 14.0118 - acc: 0.1307 - val_loss: 14.1033 - val_acc: 0.1250\n",
      "Epoch 51/80\n",
      "880/870 [==============================] - 66s - loss: 13.9385 - acc: 0.1352 - val_loss: 13.4557 - val_acc: 0.1652\n",
      "Epoch 52/80\n",
      "880/870 [==============================] - 66s - loss: 13.6637 - acc: 0.1523 - val_loss: 13.1679 - val_acc: 0.1830\n",
      "Epoch 53/80\n",
      "880/870 [==============================] - 66s - loss: 13.8469 - acc: 0.1409 - val_loss: 13.6716 - val_acc: 0.1518\n",
      "Epoch 54/80\n",
      "880/870 [==============================] - 66s - loss: 13.6637 - acc: 0.1523 - val_loss: 13.8155 - val_acc: 0.1429\n",
      "Epoch 55/80\n",
      "880/870 [==============================] - 66s - loss: 13.8286 - acc: 0.1420 - val_loss: 14.1033 - val_acc: 0.1250\n",
      "Epoch 56/80\n",
      "880/870 [==============================] - 66s - loss: 13.6637 - acc: 0.1523 - val_loss: 14.3912 - val_acc: 0.1071\n",
      "Epoch 57/80\n",
      "880/870 [==============================] - 66s - loss: 13.8469 - acc: 0.1409 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 58/80\n",
      "880/870 [==============================] - 65s - loss: 13.8286 - acc: 0.1420 - val_loss: 13.5996 - val_acc: 0.1562\n",
      "Epoch 59/80\n",
      "873/870 [==============================] - 65s - loss: 14.1980 - acc: 0.1191 - val_loss: 13.8155 - val_acc: 0.1429\n",
      "Epoch 60/80\n",
      "880/870 [==============================] - 65s - loss: 14.0667 - acc: 0.1273 - val_loss: 13.5277 - val_acc: 0.1607\n",
      "Epoch 61/80\n",
      "880/870 [==============================] - 65s - loss: 13.8286 - acc: 0.1420 - val_loss: 14.2472 - val_acc: 0.1161\n",
      "Epoch 62/80\n",
      "880/870 [==============================] - 65s - loss: 13.8286 - acc: 0.1420 - val_loss: 13.9594 - val_acc: 0.1339\n",
      "Epoch 63/80\n",
      "880/870 [==============================] - 65s - loss: 13.7553 - acc: 0.1466 - val_loss: 14.1033 - val_acc: 0.1250\n",
      "Epoch 64/80\n",
      "880/870 [==============================] - 65s - loss: 13.5539 - acc: 0.1591 - val_loss: 13.9594 - val_acc: 0.1339\n",
      "Epoch 65/80\n",
      "880/870 [==============================] - 65s - loss: 13.8835 - acc: 0.1386 - val_loss: 13.8155 - val_acc: 0.1429\n",
      "Epoch 66/80\n",
      "880/870 [==============================] - 65s - loss: 13.9934 - acc: 0.1318 - val_loss: 14.1033 - val_acc: 0.1250\n",
      "Epoch 67/80\n",
      "880/870 [==============================] - 65s - loss: 13.5905 - acc: 0.1568 - val_loss: 14.1753 - val_acc: 0.1205\n",
      "Epoch 68/80\n",
      "880/870 [==============================] - 65s - loss: 14.0850 - acc: 0.1261 - val_loss: 13.6716 - val_acc: 0.1518\n",
      "Epoch 69/80\n",
      "880/870 [==============================] - 66s - loss: 13.5905 - acc: 0.1568 - val_loss: 13.6716 - val_acc: 0.1518\n",
      "Epoch 70/80\n",
      "880/870 [==============================] - 65s - loss: 13.9385 - acc: 0.1352 - val_loss: 13.9594 - val_acc: 0.1339\n",
      "Epoch 71/80\n",
      "880/870 [==============================] - 65s - loss: 14.2315 - acc: 0.1170 - val_loss: 13.7436 - val_acc: 0.1473\n",
      "Epoch 72/80\n",
      "880/870 [==============================] - 65s - loss: 14.1583 - acc: 0.1216 - val_loss: 13.3118 - val_acc: 0.1741\n",
      "Epoch 73/80\n",
      "880/870 [==============================] - 65s - loss: 14.0484 - acc: 0.1284 - val_loss: 14.1123 - val_acc: 0.1244\n",
      "Epoch 74/80\n",
      "880/870 [==============================] - 65s - loss: 13.8286 - acc: 0.1420 - val_loss: 13.6716 - val_acc: 0.1518\n",
      "Epoch 75/80\n",
      "873/870 [==============================] - 65s - loss: 13.7918 - acc: 0.1443 - val_loss: 13.8875 - val_acc: 0.1384\n",
      "Epoch 76/80\n",
      "880/870 [==============================] - 65s - loss: 13.8652 - acc: 0.1398 - val_loss: 13.3838 - val_acc: 0.1696\n",
      "Epoch 77/80\n",
      "880/870 [==============================] - 65s - loss: 13.8103 - acc: 0.1432 - val_loss: 13.8155 - val_acc: 0.1429\n",
      "Epoch 78/80\n",
      "880/870 [==============================] - 65s - loss: 14.0850 - acc: 0.1261 - val_loss: 13.5996 - val_acc: 0.1562\n",
      "Epoch 79/80\n",
      "880/870 [==============================] - 65s - loss: 14.0301 - acc: 0.1295 - val_loss: 14.1753 - val_acc: 0.1205\n",
      "Epoch 80/80\n",
      "880/870 [==============================] - 65s - loss: 13.5355 - acc: 0.1602 - val_loss: 13.9594 - val_acc: 0.1339\n",
      "Whole model training done\n"
     ]
    }
   ],
   "source": [
    "print \"Training VGG19\"\n",
    "categorizer = Categorizer(len(CLASS_NAMES)). \\\n",
    "    build_model(VGG19). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        CLASS_NAMES.values(),\n",
    "        train_data_dir,\n",
    "        validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/VGG19.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/VGG19'\n",
    "    )\n",
    "del categorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Xception\n",
      "Found 13929 images belonging to 8 classes.\n",
      "Found 3489 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/20\n",
      "880/870 [==============================] - 37s - loss: 10.3835 - acc: 0.3057 - val_loss: 10.1091 - val_acc: 0.3571\n",
      "Epoch 2/20\n",
      "880/870 [==============================] - 38s - loss: 9.1187 - acc: 0.3955 - val_loss: 8.2808 - val_acc: 0.4241\n",
      "Epoch 3/20\n",
      "880/870 [==============================] - 38s - loss: 8.6435 - acc: 0.4057 - val_loss: 7.5051 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "880/870 [==============================] - 34s - loss: 8.7242 - acc: 0.4102 - val_loss: 7.3403 - val_acc: 0.4688\n",
      "Epoch 5/20\n",
      "880/870 [==============================] - 34s - loss: 7.7521 - acc: 0.4523 - val_loss: 7.3592 - val_acc: 0.4554\n",
      "Epoch 6/20\n",
      "880/870 [==============================] - 35s - loss: 7.7618 - acc: 0.4455 - val_loss: 7.5866 - val_acc: 0.4509\n",
      "Epoch 7/20\n",
      "880/870 [==============================] - 40s - loss: 6.9938 - acc: 0.4534 - val_loss: 4.9083 - val_acc: 0.5491\n",
      "Epoch 8/20\n",
      "880/870 [==============================] - 38s - loss: 3.7040 - acc: 0.4875 - val_loss: 1.4413 - val_acc: 0.5759\n",
      "Epoch 9/20\n",
      "880/870 [==============================] - 38s - loss: 1.5628 - acc: 0.5239 - val_loss: 1.3041 - val_acc: 0.6116\n",
      "Epoch 10/20\n",
      "880/870 [==============================] - 38s - loss: 1.4467 - acc: 0.5261 - val_loss: 1.3029 - val_acc: 0.6161\n",
      "Epoch 11/20\n",
      "880/870 [==============================] - 34s - loss: 1.3568 - acc: 0.5216 - val_loss: 1.2447 - val_acc: 0.6027\n",
      "Epoch 12/20\n",
      "880/870 [==============================] - 38s - loss: 1.3917 - acc: 0.5364 - val_loss: 1.0128 - val_acc: 0.6830\n",
      "Epoch 13/20\n",
      "880/870 [==============================] - 34s - loss: 1.3848 - acc: 0.5489 - val_loss: 1.2067 - val_acc: 0.6161\n",
      "Epoch 14/20\n",
      "880/870 [==============================] - 33s - loss: 1.2698 - acc: 0.5761 - val_loss: 1.1622 - val_acc: 0.6295\n",
      "Epoch 15/20\n",
      "880/870 [==============================] - 34s - loss: 1.2715 - acc: 0.5614 - val_loss: 0.9222 - val_acc: 0.6830\n",
      "Epoch 16/20\n",
      "873/870 [==============================] - 33s - loss: 1.2814 - acc: 0.5590 - val_loss: 1.0966 - val_acc: 0.6384\n",
      "Epoch 17/20\n",
      "880/870 [==============================] - 33s - loss: 1.2476 - acc: 0.5739 - val_loss: 1.0181 - val_acc: 0.6116\n",
      "Epoch 18/20\n",
      "880/870 [==============================] - 33s - loss: 1.2078 - acc: 0.5750 - val_loss: 0.9627 - val_acc: 0.6741\n",
      "Epoch 19/20\n",
      "880/870 [==============================] - 38s - loss: 1.2240 - acc: 0.5693 - val_loss: 0.9835 - val_acc: 0.6964\n",
      "Epoch 20/20\n",
      "880/870 [==============================] - 34s - loss: 1.2475 - acc: 0.5784 - val_loss: 0.9642 - val_acc: 0.6622\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/80\n",
      "880/870 [==============================] - 94s - loss: 1.5624 - acc: 0.5193 - val_loss: 1.3926 - val_acc: 0.6339\n",
      "Epoch 2/80\n",
      "880/870 [==============================] - 93s - loss: 1.1565 - acc: 0.6216 - val_loss: 1.1164 - val_acc: 0.6786\n",
      "Epoch 3/80\n",
      "880/870 [==============================] - 93s - loss: 1.0694 - acc: 0.6659 - val_loss: 0.8797 - val_acc: 0.7009\n",
      "Epoch 4/80\n",
      "880/870 [==============================] - 88s - loss: 0.9863 - acc: 0.6875 - val_loss: 0.9891 - val_acc: 0.7009\n",
      "Epoch 5/80\n",
      "880/870 [==============================] - 89s - loss: 0.9697 - acc: 0.6886 - val_loss: 0.9410 - val_acc: 0.6786\n",
      "Epoch 6/80\n",
      "880/870 [==============================] - 94s - loss: 0.9420 - acc: 0.6818 - val_loss: 0.8547 - val_acc: 0.7188\n",
      "Epoch 7/80\n",
      "880/870 [==============================] - 94s - loss: 0.8799 - acc: 0.6932 - val_loss: 0.7073 - val_acc: 0.7500\n",
      "Epoch 8/80\n",
      "880/870 [==============================] - 88s - loss: 0.8606 - acc: 0.7125 - val_loss: 0.7384 - val_acc: 0.7188\n",
      "Epoch 9/80\n",
      "880/870 [==============================] - 87s - loss: 0.8826 - acc: 0.7182 - val_loss: 0.7803 - val_acc: 0.7143\n",
      "Epoch 10/80\n",
      "880/870 [==============================] - 87s - loss: 0.9036 - acc: 0.7000 - val_loss: 0.7973 - val_acc: 0.7333\n",
      "Epoch 11/80\n",
      "880/870 [==============================] - 87s - loss: 0.8028 - acc: 0.7193 - val_loss: 0.7377 - val_acc: 0.7455\n",
      "Epoch 12/80\n",
      "873/870 [==============================] - 87s - loss: 0.7474 - acc: 0.7549 - val_loss: 0.7792 - val_acc: 0.7321\n",
      "Epoch 13/80\n",
      "880/870 [==============================] - 92s - loss: 0.6690 - acc: 0.7864 - val_loss: 0.8212 - val_acc: 0.7589\n",
      "Epoch 14/80\n",
      "880/870 [==============================] - 87s - loss: 0.5966 - acc: 0.8239 - val_loss: 0.8325 - val_acc: 0.7232\n",
      "Epoch 15/80\n",
      "880/870 [==============================] - 87s - loss: 0.6813 - acc: 0.7784 - val_loss: 0.7483 - val_acc: 0.7545\n",
      "Epoch 16/80\n",
      "880/870 [==============================] - 94s - loss: 0.6778 - acc: 0.7727 - val_loss: 0.6237 - val_acc: 0.8080\n",
      "Epoch 17/80\n",
      "880/870 [==============================] - 87s - loss: 0.6937 - acc: 0.7773 - val_loss: 0.6390 - val_acc: 0.7991\n",
      "Epoch 18/80\n",
      "880/870 [==============================] - 87s - loss: 0.7339 - acc: 0.7534 - val_loss: 0.6758 - val_acc: 0.7991\n",
      "Epoch 19/80\n",
      "880/870 [==============================] - 87s - loss: 0.5954 - acc: 0.8057 - val_loss: 0.7497 - val_acc: 0.7467\n",
      "Epoch 20/80\n",
      "880/870 [==============================] - 88s - loss: 0.5894 - acc: 0.8068 - val_loss: 0.7298 - val_acc: 0.7277\n",
      "Epoch 21/80\n",
      "880/870 [==============================] - 88s - loss: 0.6905 - acc: 0.7670 - val_loss: 0.6816 - val_acc: 0.7812\n",
      "Epoch 22/80\n",
      "880/870 [==============================] - 88s - loss: 0.6690 - acc: 0.7852 - val_loss: 0.8336 - val_acc: 0.7411\n",
      "Epoch 23/80\n",
      "880/870 [==============================] - 87s - loss: 0.6344 - acc: 0.7898 - val_loss: 0.8007 - val_acc: 0.7321\n",
      "Epoch 24/80\n",
      "880/870 [==============================] - 87s - loss: 0.5894 - acc: 0.7943 - val_loss: 0.5657 - val_acc: 0.7946\n",
      "Epoch 25/80\n",
      "880/870 [==============================] - 87s - loss: 0.6080 - acc: 0.7830 - val_loss: 0.6710 - val_acc: 0.7500\n",
      "Epoch 26/80\n",
      "880/870 [==============================] - 87s - loss: 0.6683 - acc: 0.7761 - val_loss: 0.7669 - val_acc: 0.7768\n",
      "Epoch 27/80\n",
      "880/870 [==============================] - 88s - loss: 0.6289 - acc: 0.7852 - val_loss: 0.6715 - val_acc: 0.7768\n",
      "Epoch 28/80\n",
      "873/870 [==============================] - 92s - loss: 0.5804 - acc: 0.8121 - val_loss: 0.5140 - val_acc: 0.8393\n",
      "Epoch 29/80\n",
      "880/870 [==============================] - 87s - loss: 0.5167 - acc: 0.8239 - val_loss: 0.5347 - val_acc: 0.8222\n",
      "Epoch 30/80\n",
      "880/870 [==============================] - 87s - loss: 0.4620 - acc: 0.8477 - val_loss: 0.6949 - val_acc: 0.7768\n",
      "Epoch 31/80\n",
      "880/870 [==============================] - 87s - loss: 0.4821 - acc: 0.8239 - val_loss: 0.7246 - val_acc: 0.7634\n",
      "Epoch 32/80\n",
      "880/870 [==============================] - 87s - loss: 0.4573 - acc: 0.8398 - val_loss: 0.6394 - val_acc: 0.8214\n",
      "Epoch 33/80\n",
      "880/870 [==============================] - 87s - loss: 0.4313 - acc: 0.8477 - val_loss: 0.8226 - val_acc: 0.7768\n",
      "Epoch 34/80\n",
      "880/870 [==============================] - 87s - loss: 0.4864 - acc: 0.8341 - val_loss: 0.7401 - val_acc: 0.7411\n",
      "Epoch 35/80\n",
      "880/870 [==============================] - 88s - loss: 0.4603 - acc: 0.8455 - val_loss: 0.7090 - val_acc: 0.7679\n",
      "Epoch 36/80\n",
      "880/870 [==============================] - 87s - loss: 0.4471 - acc: 0.8443 - val_loss: 0.8569 - val_acc: 0.7545\n",
      "Epoch 37/80\n",
      "880/870 [==============================] - 88s - loss: 0.4985 - acc: 0.8341 - val_loss: 0.7429 - val_acc: 0.7500\n",
      "Epoch 38/80\n",
      "880/870 [==============================] - 89s - loss: 0.4133 - acc: 0.8602 - val_loss: 0.6565 - val_acc: 0.7911\n",
      "Epoch 39/80\n",
      "880/870 [==============================] - 88s - loss: 0.4670 - acc: 0.8295 - val_loss: 0.7543 - val_acc: 0.7545\n",
      "Epoch 40/80\n",
      "880/870 [==============================] - 87s - loss: 0.4853 - acc: 0.8455 - val_loss: 0.8119 - val_acc: 0.7500\n",
      "Epoch 41/80\n",
      "880/870 [==============================] - 87s - loss: 0.4535 - acc: 0.8443 - val_loss: 0.6325 - val_acc: 0.7946\n",
      "Epoch 42/80\n",
      "880/870 [==============================] - 87s - loss: 0.4247 - acc: 0.8511 - val_loss: 0.5710 - val_acc: 0.7902\n",
      "Epoch 43/80\n",
      "880/870 [==============================] - 87s - loss: 0.5021 - acc: 0.8273 - val_loss: 0.7302 - val_acc: 0.7768\n",
      "Epoch 44/80\n",
      "873/870 [==============================] - 87s - loss: 0.4345 - acc: 0.8603 - val_loss: 0.7151 - val_acc: 0.7768\n",
      "Epoch 45/80\n",
      "880/870 [==============================] - 87s - loss: 0.3533 - acc: 0.8750 - val_loss: 0.7176 - val_acc: 0.7812\n",
      "Epoch 46/80\n",
      "880/870 [==============================] - 87s - loss: 0.3720 - acc: 0.8886 - val_loss: 0.7670 - val_acc: 0.7589\n",
      "Epoch 47/80\n",
      "880/870 [==============================] - 87s - loss: 0.3835 - acc: 0.8591 - val_loss: 0.7766 - val_acc: 0.7768\n",
      "Epoch 48/80\n",
      "880/870 [==============================] - 87s - loss: 0.3858 - acc: 0.8591 - val_loss: 0.7319 - val_acc: 0.7589\n",
      "Epoch 49/80\n",
      "880/870 [==============================] - 88s - loss: 0.2994 - acc: 0.8943 - val_loss: 0.6493 - val_acc: 0.8170\n",
      "Epoch 50/80\n",
      "880/870 [==============================] - 87s - loss: 0.3499 - acc: 0.8705 - val_loss: 0.6902 - val_acc: 0.7768\n",
      "Epoch 51/80\n",
      "880/870 [==============================] - 87s - loss: 0.3298 - acc: 0.8750 - val_loss: 0.7672 - val_acc: 0.7545\n",
      "Epoch 52/80\n",
      "880/870 [==============================] - 87s - loss: 0.3284 - acc: 0.8898 - val_loss: 0.8834 - val_acc: 0.7589\n",
      "Epoch 53/80\n",
      "880/870 [==============================] - 87s - loss: 0.3318 - acc: 0.8875 - val_loss: 0.8084 - val_acc: 0.7946\n",
      "Epoch 54/80\n",
      "880/870 [==============================] - 87s - loss: 0.4103 - acc: 0.8614 - val_loss: 0.6700 - val_acc: 0.7902\n",
      "Epoch 55/80\n",
      "880/870 [==============================] - 88s - loss: 0.3519 - acc: 0.8773 - val_loss: 0.8785 - val_acc: 0.7500\n",
      "Epoch 56/80\n",
      "880/870 [==============================] - 88s - loss: 0.3539 - acc: 0.8818 - val_loss: 0.7184 - val_acc: 0.8125\n",
      "Epoch 57/80\n",
      "880/870 [==============================] - 88s - loss: 0.3811 - acc: 0.8557 - val_loss: 0.9015 - val_acc: 0.7812\n",
      "Epoch 58/80\n",
      "880/870 [==============================] - 87s - loss: 0.3411 - acc: 0.8898 - val_loss: 0.7356 - val_acc: 0.7902\n",
      "Epoch 59/80\n",
      "873/870 [==============================] - 87s - loss: 0.3232 - acc: 0.8958 - val_loss: 0.6373 - val_acc: 0.7991\n",
      "Epoch 60/80\n",
      "880/870 [==============================] - 94s - loss: 0.2747 - acc: 0.9023 - val_loss: 0.5162 - val_acc: 0.8482\n",
      "Epoch 61/80\n",
      "880/870 [==============================] - 88s - loss: 0.2490 - acc: 0.9205 - val_loss: 0.7383 - val_acc: 0.7902\n",
      "Epoch 62/80\n",
      "880/870 [==============================] - 88s - loss: 0.2604 - acc: 0.9068 - val_loss: 0.8247 - val_acc: 0.7545\n",
      "Epoch 63/80\n",
      "880/870 [==============================] - 88s - loss: 0.2875 - acc: 0.9114 - val_loss: 0.9403 - val_acc: 0.7768\n",
      "Epoch 64/80\n",
      "880/870 [==============================] - 88s - loss: 0.2702 - acc: 0.9057 - val_loss: 0.7850 - val_acc: 0.7946\n",
      "Epoch 65/80\n",
      "880/870 [==============================] - 88s - loss: 0.2883 - acc: 0.9057 - val_loss: 0.9026 - val_acc: 0.7812\n",
      "Epoch 66/80\n",
      "880/870 [==============================] - 88s - loss: 0.2992 - acc: 0.9057 - val_loss: 0.7410 - val_acc: 0.7991\n",
      "Epoch 67/80\n",
      "880/870 [==============================] - 88s - loss: 0.2619 - acc: 0.9114 - val_loss: 0.8667 - val_acc: 0.7679\n",
      "Epoch 68/80\n",
      "880/870 [==============================] - 88s - loss: 0.2798 - acc: 0.9080 - val_loss: 0.8937 - val_acc: 0.7589\n",
      "Epoch 69/80\n",
      "880/870 [==============================] - 88s - loss: 0.2736 - acc: 0.9136 - val_loss: 0.9194 - val_acc: 0.7679\n",
      "Epoch 70/80\n",
      "880/870 [==============================] - 90s - loss: 0.2714 - acc: 0.9011 - val_loss: 0.6992 - val_acc: 0.8080\n",
      "Epoch 71/80\n",
      "880/870 [==============================] - 89s - loss: 0.2656 - acc: 0.9034 - val_loss: 0.8595 - val_acc: 0.7098\n",
      "Epoch 72/80\n",
      "880/870 [==============================] - 88s - loss: 0.2887 - acc: 0.8989 - val_loss: 0.8466 - val_acc: 0.7679\n",
      "Epoch 73/80\n",
      "880/870 [==============================] - 88s - loss: 0.2837 - acc: 0.9125 - val_loss: 0.5861 - val_acc: 0.8393\n",
      "Epoch 74/80\n",
      "880/870 [==============================] - 88s - loss: 0.2754 - acc: 0.9011 - val_loss: 0.9232 - val_acc: 0.7634\n",
      "Epoch 75/80\n",
      "873/870 [==============================] - 88s - loss: 0.2717 - acc: 0.9072 - val_loss: 0.8500 - val_acc: 0.7956\n",
      "Epoch 76/80\n",
      "880/870 [==============================] - 88s - loss: 0.2000 - acc: 0.9330 - val_loss: 0.7739 - val_acc: 0.7812\n",
      "Epoch 77/80\n",
      "880/870 [==============================] - 88s - loss: 0.2242 - acc: 0.9205 - val_loss: 0.8765 - val_acc: 0.7768\n",
      "Epoch 78/80\n",
      "880/870 [==============================] - 88s - loss: 0.1935 - acc: 0.9375 - val_loss: 0.8812 - val_acc: 0.7679\n",
      "Epoch 79/80\n",
      "880/870 [==============================] - 88s - loss: 0.2124 - acc: 0.9284 - val_loss: 0.9185 - val_acc: 0.7634\n",
      "Epoch 80/80\n",
      "880/870 [==============================] - 88s - loss: 0.2423 - acc: 0.9227 - val_loss: 1.1097 - val_acc: 0.7545\n",
      "Whole model training done\n"
     ]
    }
   ],
   "source": [
    "print \"Training Xception\"\n",
    "categorizer = Categorizer(len(CLASS_NAMES)). \\\n",
    "    build_model(Xception). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        CLASS_NAMES.values(),\n",
    "        train_data_dir,\n",
    "        validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/Xception.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/Xception'\n",
    "    )\n",
    "del categorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training InceptionV3\n",
      "Found 13929 images belonging to 8 classes.\n",
      "Found 3489 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/20\n",
      "880/870 [==============================] - 65s - loss: 9.0356 - acc: 0.3511 - val_loss: 8.7741 - val_acc: 0.3839\n",
      "Epoch 2/20\n",
      "880/870 [==============================] - 26s - loss: 8.0852 - acc: 0.4227 - val_loss: 7.9654 - val_acc: 0.4375\n",
      "Epoch 3/20\n",
      "880/870 [==============================] - 26s - loss: 7.6998 - acc: 0.4534 - val_loss: 6.9515 - val_acc: 0.5045\n",
      "Epoch 4/20\n",
      "880/870 [==============================] - 26s - loss: 7.1624 - acc: 0.4830 - val_loss: 6.4973 - val_acc: 0.5312\n",
      "Epoch 5/20\n",
      "880/870 [==============================] - 26s - loss: 7.0213 - acc: 0.4966 - val_loss: 6.2507 - val_acc: 0.5491\n",
      "Epoch 6/20\n",
      "880/870 [==============================] - 22s - loss: 7.3577 - acc: 0.4761 - val_loss: 6.9894 - val_acc: 0.4911\n",
      "Epoch 7/20\n",
      "880/870 [==============================] - 26s - loss: 7.0274 - acc: 0.4761 - val_loss: 5.2872 - val_acc: 0.6027\n",
      "Epoch 8/20\n",
      "880/870 [==============================] - 23s - loss: 6.9674 - acc: 0.4750 - val_loss: 6.5707 - val_acc: 0.5312\n",
      "Epoch 9/20\n",
      "880/870 [==============================] - 23s - loss: 6.2481 - acc: 0.5341 - val_loss: 6.3462 - val_acc: 0.5179\n",
      "Epoch 10/20\n",
      "880/870 [==============================] - 22s - loss: 5.7850 - acc: 0.5420 - val_loss: 4.9192 - val_acc: 0.5848\n",
      "Epoch 11/20\n",
      "880/870 [==============================] - 23s - loss: 6.2608 - acc: 0.5182 - val_loss: 4.7445 - val_acc: 0.5982\n",
      "Epoch 12/20\n",
      "880/870 [==============================] - 26s - loss: 6.0818 - acc: 0.5091 - val_loss: 4.3657 - val_acc: 0.6295\n",
      "Epoch 13/20\n",
      "880/870 [==============================] - 26s - loss: 5.3522 - acc: 0.5670 - val_loss: 3.6760 - val_acc: 0.6652\n",
      "Epoch 14/20\n",
      "880/870 [==============================] - 26s - loss: 5.6773 - acc: 0.5295 - val_loss: 3.7833 - val_acc: 0.6667\n",
      "Epoch 15/20\n",
      "880/870 [==============================] - 23s - loss: 5.1691 - acc: 0.5523 - val_loss: 4.4268 - val_acc: 0.6295\n",
      "Epoch 16/20\n",
      "873/870 [==============================] - 23s - loss: 5.4451 - acc: 0.5166 - val_loss: 3.9167 - val_acc: 0.6384\n",
      "Epoch 17/20\n",
      "880/870 [==============================] - 23s - loss: 4.4352 - acc: 0.6000 - val_loss: 4.3378 - val_acc: 0.6429\n",
      "Epoch 18/20\n",
      "880/870 [==============================] - 22s - loss: 4.3333 - acc: 0.5943 - val_loss: 3.5662 - val_acc: 0.6473\n",
      "Epoch 19/20\n",
      "880/870 [==============================] - 23s - loss: 4.0112 - acc: 0.5966 - val_loss: 3.7050 - val_acc: 0.6384\n",
      "Epoch 20/20\n",
      "880/870 [==============================] - 22s - loss: 3.5214 - acc: 0.6045 - val_loss: 3.2446 - val_acc: 0.6295\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/80\n",
      "880/870 [==============================] - 66s - loss: 8.0364 - acc: 0.4023 - val_loss: 7.4374 - val_acc: 0.4509\n",
      "Epoch 2/80\n",
      "880/870 [==============================] - 61s - loss: 7.1021 - acc: 0.4523 - val_loss: 6.0032 - val_acc: 0.5179\n",
      "Epoch 3/80\n",
      "880/870 [==============================] - 60s - loss: 6.6501 - acc: 0.4659 - val_loss: 5.6492 - val_acc: 0.5714\n",
      "Epoch 4/80\n",
      "880/870 [==============================] - 60s - loss: 5.9585 - acc: 0.4898 - val_loss: 4.8345 - val_acc: 0.5982\n",
      "Epoch 5/80\n",
      "880/870 [==============================] - 56s - loss: 4.1087 - acc: 0.5159 - val_loss: 3.1134 - val_acc: 0.5357\n",
      "Epoch 6/80\n",
      "880/870 [==============================] - 56s - loss: 2.7253 - acc: 0.4682 - val_loss: 2.6558 - val_acc: 0.4821\n",
      "Epoch 7/80\n",
      "880/870 [==============================] - 60s - loss: 1.9956 - acc: 0.4591 - val_loss: 1.8082 - val_acc: 0.5422\n",
      "Epoch 8/80\n",
      "880/870 [==============================] - 57s - loss: 1.8406 - acc: 0.4045 - val_loss: 1.4522 - val_acc: 0.5670\n",
      "Epoch 9/80\n",
      "880/870 [==============================] - 56s - loss: 1.7986 - acc: 0.4273 - val_loss: 1.6134 - val_acc: 0.4911\n",
      "Epoch 10/80\n",
      "880/870 [==============================] - 56s - loss: 1.7124 - acc: 0.4091 - val_loss: 1.2428 - val_acc: 0.5848\n",
      "Epoch 11/80\n",
      "880/870 [==============================] - 56s - loss: 1.6323 - acc: 0.4364 - val_loss: 1.7937 - val_acc: 0.5580\n",
      "Epoch 12/80\n",
      "873/870 [==============================] - 57s - loss: 1.6066 - acc: 0.4685 - val_loss: 1.7264 - val_acc: 0.5268\n",
      "Epoch 13/80\n",
      "880/870 [==============================] - 56s - loss: 1.5452 - acc: 0.4807 - val_loss: 1.3833 - val_acc: 0.5580\n",
      "Epoch 14/80\n",
      "880/870 [==============================] - 56s - loss: 1.4954 - acc: 0.5125 - val_loss: 1.6277 - val_acc: 0.5848\n",
      "Epoch 15/80\n",
      "880/870 [==============================] - 56s - loss: 1.4519 - acc: 0.5364 - val_loss: 1.5064 - val_acc: 0.5670\n",
      "Epoch 16/80\n",
      "880/870 [==============================] - 56s - loss: 1.4942 - acc: 0.5182 - val_loss: 1.2568 - val_acc: 0.5759\n",
      "Epoch 17/80\n",
      "880/870 [==============================] - 61s - loss: 1.4978 - acc: 0.4818 - val_loss: 1.3072 - val_acc: 0.6250\n",
      "Epoch 18/80\n",
      "880/870 [==============================] - 56s - loss: 1.5173 - acc: 0.5034 - val_loss: 1.4414 - val_acc: 0.5580\n",
      "Epoch 19/80\n",
      "880/870 [==============================] - 56s - loss: 1.3895 - acc: 0.5364 - val_loss: 1.7421 - val_acc: 0.5312\n",
      "Epoch 20/80\n",
      "880/870 [==============================] - 56s - loss: 1.4923 - acc: 0.5102 - val_loss: 1.3298 - val_acc: 0.5714\n",
      "Epoch 21/80\n",
      "880/870 [==============================] - 61s - loss: 1.4460 - acc: 0.5125 - val_loss: 1.2269 - val_acc: 0.6489\n",
      "Epoch 22/80\n",
      "880/870 [==============================] - 56s - loss: 1.6786 - acc: 0.4705 - val_loss: 1.3644 - val_acc: 0.6116\n",
      "Epoch 23/80\n",
      "880/870 [==============================] - 56s - loss: 1.5518 - acc: 0.4739 - val_loss: 1.3326 - val_acc: 0.5714\n",
      "Epoch 24/80\n",
      "880/870 [==============================] - 56s - loss: 1.3711 - acc: 0.5318 - val_loss: 1.4175 - val_acc: 0.5759\n",
      "Epoch 25/80\n",
      "880/870 [==============================] - 56s - loss: 1.4732 - acc: 0.5420 - val_loss: 1.1645 - val_acc: 0.6295\n",
      "Epoch 26/80\n",
      "880/870 [==============================] - 56s - loss: 1.4756 - acc: 0.5011 - val_loss: 1.3411 - val_acc: 0.6116\n",
      "Epoch 27/80\n",
      "880/870 [==============================] - 61s - loss: 1.3435 - acc: 0.5534 - val_loss: 1.1994 - val_acc: 0.6518\n",
      "Epoch 28/80\n",
      "873/870 [==============================] - 56s - loss: 1.3912 - acc: 0.5349 - val_loss: 1.5054 - val_acc: 0.5714\n",
      "Epoch 29/80\n",
      "880/870 [==============================] - 61s - loss: 1.3780 - acc: 0.5239 - val_loss: 1.2688 - val_acc: 0.6607\n",
      "Epoch 30/80\n",
      "880/870 [==============================] - 56s - loss: 1.3469 - acc: 0.5614 - val_loss: 1.2502 - val_acc: 0.6295\n",
      "Epoch 31/80\n",
      "880/870 [==============================] - 56s - loss: 1.2852 - acc: 0.5670 - val_loss: 1.3374 - val_acc: 0.5804\n",
      "Epoch 32/80\n",
      "880/870 [==============================] - 56s - loss: 1.3060 - acc: 0.5830 - val_loss: 1.4100 - val_acc: 0.6384\n",
      "Epoch 33/80\n",
      "880/870 [==============================] - 56s - loss: 1.2152 - acc: 0.5693 - val_loss: 1.0740 - val_acc: 0.6339\n",
      "Epoch 34/80\n",
      "880/870 [==============================] - 60s - loss: 1.3641 - acc: 0.5500 - val_loss: 1.2123 - val_acc: 0.6533\n",
      "Epoch 35/80\n",
      "880/870 [==============================] - 56s - loss: 1.3161 - acc: 0.5432 - val_loss: 1.1999 - val_acc: 0.6250\n",
      "Epoch 36/80\n",
      "880/870 [==============================] - 56s - loss: 1.3116 - acc: 0.5386 - val_loss: 1.5066 - val_acc: 0.6295\n",
      "Epoch 37/80\n",
      "880/870 [==============================] - 61s - loss: 1.3106 - acc: 0.5614 - val_loss: 1.1973 - val_acc: 0.6696\n",
      "Epoch 38/80\n",
      "880/870 [==============================] - 56s - loss: 1.2761 - acc: 0.5659 - val_loss: 1.2549 - val_acc: 0.5938\n",
      "Epoch 39/80\n",
      "880/870 [==============================] - 56s - loss: 1.1864 - acc: 0.5818 - val_loss: 1.4127 - val_acc: 0.6250\n",
      "Epoch 40/80\n",
      "880/870 [==============================] - 56s - loss: 1.3183 - acc: 0.5591 - val_loss: 1.1564 - val_acc: 0.6250\n",
      "Epoch 41/80\n",
      "880/870 [==============================] - 56s - loss: 1.2860 - acc: 0.5386 - val_loss: 1.1488 - val_acc: 0.6250\n",
      "Epoch 42/80\n",
      "880/870 [==============================] - 56s - loss: 1.3082 - acc: 0.5739 - val_loss: 1.2954 - val_acc: 0.6562\n",
      "Epoch 43/80\n",
      "880/870 [==============================] - 56s - loss: 1.3027 - acc: 0.5602 - val_loss: 1.3217 - val_acc: 0.6652\n",
      "Epoch 44/80\n",
      "873/870 [==============================] - 56s - loss: 1.1459 - acc: 0.5956 - val_loss: 1.4651 - val_acc: 0.6205\n",
      "Epoch 45/80\n",
      "880/870 [==============================] - 56s - loss: 1.1472 - acc: 0.5977 - val_loss: 1.1812 - val_acc: 0.6384\n",
      "Epoch 46/80\n",
      "880/870 [==============================] - 60s - loss: 1.3018 - acc: 0.5761 - val_loss: 1.2377 - val_acc: 0.6741\n",
      "Epoch 47/80\n",
      "880/870 [==============================] - 61s - loss: 1.1510 - acc: 0.5864 - val_loss: 1.0308 - val_acc: 0.6830\n",
      "Epoch 48/80\n",
      "880/870 [==============================] - 56s - loss: 1.2741 - acc: 0.5568 - val_loss: 1.5045 - val_acc: 0.6444\n",
      "Epoch 49/80\n",
      "880/870 [==============================] - 56s - loss: 1.2068 - acc: 0.5841 - val_loss: 1.2956 - val_acc: 0.6652\n",
      "Epoch 50/80\n",
      "880/870 [==============================] - 59s - loss: 1.1499 - acc: 0.5898 - val_loss: 1.0609 - val_acc: 0.6607\n",
      "Epoch 51/80\n",
      "880/870 [==============================] - 62s - loss: 1.2700 - acc: 0.5716 - val_loss: 1.0122 - val_acc: 0.6920\n",
      "Epoch 52/80\n",
      "880/870 [==============================] - 56s - loss: 1.1582 - acc: 0.5875 - val_loss: 1.1494 - val_acc: 0.5848\n",
      "Epoch 53/80\n",
      "880/870 [==============================] - 56s - loss: 1.2126 - acc: 0.5761 - val_loss: 1.2940 - val_acc: 0.6607\n",
      "Epoch 54/80\n",
      "880/870 [==============================] - 56s - loss: 1.2878 - acc: 0.5864 - val_loss: 1.4093 - val_acc: 0.6384\n",
      "Epoch 55/80\n",
      "880/870 [==============================] - 56s - loss: 1.1263 - acc: 0.5989 - val_loss: 1.0614 - val_acc: 0.6518\n",
      "Epoch 56/80\n",
      "880/870 [==============================] - 57s - loss: 1.2312 - acc: 0.6023 - val_loss: 1.4950 - val_acc: 0.5982\n",
      "Epoch 57/80\n",
      "880/870 [==============================] - 60s - loss: 1.1460 - acc: 0.6182 - val_loss: 1.1364 - val_acc: 0.6964\n",
      "Epoch 58/80\n",
      "880/870 [==============================] - 56s - loss: 1.1703 - acc: 0.5852 - val_loss: 1.0543 - val_acc: 0.6473\n",
      "Epoch 59/80\n",
      "873/870 [==============================] - 60s - loss: 1.1698 - acc: 0.6060 - val_loss: 1.1526 - val_acc: 0.7321\n",
      "Epoch 60/80\n",
      "880/870 [==============================] - 56s - loss: 1.0777 - acc: 0.6023 - val_loss: 1.1607 - val_acc: 0.6786\n",
      "Epoch 61/80\n",
      "880/870 [==============================] - 56s - loss: 1.0669 - acc: 0.6136 - val_loss: 1.0916 - val_acc: 0.7143\n",
      "Epoch 62/80\n",
      "880/870 [==============================] - 56s - loss: 1.0231 - acc: 0.6386 - val_loss: 1.1248 - val_acc: 0.6844\n",
      "Epoch 63/80\n",
      "880/870 [==============================] - 56s - loss: 1.2095 - acc: 0.5898 - val_loss: 0.9750 - val_acc: 0.7232\n",
      "Epoch 64/80\n",
      "880/870 [==============================] - 56s - loss: 1.0982 - acc: 0.6341 - val_loss: 1.0146 - val_acc: 0.6920\n",
      "Epoch 65/80\n",
      "880/870 [==============================] - 56s - loss: 1.1179 - acc: 0.6091 - val_loss: 1.1529 - val_acc: 0.6607\n",
      "Epoch 66/80\n",
      "880/870 [==============================] - 56s - loss: 1.0872 - acc: 0.6284 - val_loss: 0.9847 - val_acc: 0.6518\n",
      "Epoch 67/80\n",
      "880/870 [==============================] - 58s - loss: 1.1032 - acc: 0.6409 - val_loss: 1.1267 - val_acc: 0.6652\n",
      "Epoch 68/80\n",
      "880/870 [==============================] - 56s - loss: 1.0544 - acc: 0.6125 - val_loss: 1.3126 - val_acc: 0.6295\n",
      "Epoch 69/80\n",
      "880/870 [==============================] - 56s - loss: 1.1025 - acc: 0.6148 - val_loss: 1.4898 - val_acc: 0.6696\n",
      "Epoch 70/80\n",
      "880/870 [==============================] - 56s - loss: 1.0832 - acc: 0.6239 - val_loss: 1.3256 - val_acc: 0.6652\n",
      "Epoch 71/80\n",
      "880/870 [==============================] - 56s - loss: 1.0895 - acc: 0.6250 - val_loss: 1.1539 - val_acc: 0.6830\n",
      "Epoch 72/80\n",
      "880/870 [==============================] - 56s - loss: 1.1840 - acc: 0.5943 - val_loss: 1.0480 - val_acc: 0.7098\n",
      "Epoch 73/80\n",
      "880/870 [==============================] - 56s - loss: 1.0508 - acc: 0.6102 - val_loss: 1.2777 - val_acc: 0.6786\n",
      "Epoch 74/80\n",
      "880/870 [==============================] - 56s - loss: 1.1070 - acc: 0.6216 - val_loss: 1.1230 - val_acc: 0.5893\n",
      "Epoch 75/80\n",
      "873/870 [==============================] - 60s - loss: 1.1489 - acc: 0.6071 - val_loss: 0.9059 - val_acc: 0.7589\n",
      "Epoch 76/80\n",
      "880/870 [==============================] - 56s - loss: 1.0845 - acc: 0.6182 - val_loss: 1.2964 - val_acc: 0.6533\n",
      "Epoch 77/80\n",
      "880/870 [==============================] - 56s - loss: 1.0172 - acc: 0.6341 - val_loss: 0.9018 - val_acc: 0.6920\n",
      "Epoch 78/80\n",
      "880/870 [==============================] - 56s - loss: 1.0843 - acc: 0.6386 - val_loss: 0.9958 - val_acc: 0.7009\n",
      "Epoch 79/80\n",
      "880/870 [==============================] - 56s - loss: 1.0804 - acc: 0.6409 - val_loss: 1.3028 - val_acc: 0.6830\n",
      "Epoch 80/80\n",
      "880/870 [==============================] - 56s - loss: 1.0887 - acc: 0.6352 - val_loss: 1.0684 - val_acc: 0.6562\n",
      "Whole model training done\n"
     ]
    }
   ],
   "source": [
    "print \"Training InceptionV3\"\n",
    "categorizer = Categorizer(len(CLASS_NAMES)). \\\n",
    "    build_model(InceptionV3). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        CLASS_NAMES.values(),\n",
    "        train_data_dir,\n",
    "        validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/InceptionV3.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/InceptionV3'\n",
    "    )\n",
    "del categorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet50\n",
      "Found 13929 images belonging to 8 classes.\n",
      "Found 3489 images belonging to 8 classes.\n",
      "Starting with top layers training...\n",
      "Epoch 1/20\n",
      "880/870 [==============================] - 46s - loss: 1.9810 - acc: 0.3159 - val_loss: 1.9391 - val_acc: 0.1920\n",
      "Epoch 2/20\n",
      "880/870 [==============================] - 14s - loss: 1.6318 - acc: 0.4170 - val_loss: 1.9563 - val_acc: 0.1116\n",
      "Epoch 3/20\n",
      "880/870 [==============================] - 15s - loss: 1.5375 - acc: 0.4523 - val_loss: 1.9425 - val_acc: 0.2366\n",
      "Epoch 4/20\n",
      "880/870 [==============================] - 15s - loss: 1.5408 - acc: 0.4443 - val_loss: 1.9916 - val_acc: 0.2455\n",
      "Epoch 5/20\n",
      "880/870 [==============================] - 15s - loss: 1.4721 - acc: 0.4625 - val_loss: 1.9145 - val_acc: 0.3170\n",
      "Epoch 6/20\n",
      "880/870 [==============================] - 14s - loss: 1.4432 - acc: 0.4727 - val_loss: 1.9072 - val_acc: 0.2679\n",
      "Epoch 7/20\n",
      "880/870 [==============================] - 15s - loss: 1.3641 - acc: 0.5170 - val_loss: 1.8334 - val_acc: 0.3304\n",
      "Epoch 8/20\n",
      "880/870 [==============================] - 25s - loss: 1.4411 - acc: 0.4773 - val_loss: 1.8080 - val_acc: 0.3125\n",
      "Epoch 9/20\n",
      "880/870 [==============================] - 19s - loss: 1.4185 - acc: 0.4830 - val_loss: 1.7868 - val_acc: 0.3482\n",
      "Epoch 10/20\n",
      "880/870 [==============================] - 15s - loss: 1.2976 - acc: 0.5534 - val_loss: 1.6928 - val_acc: 0.3750\n",
      "Epoch 11/20\n",
      "880/870 [==============================] - 15s - loss: 1.3935 - acc: 0.4989 - val_loss: 1.5251 - val_acc: 0.4330\n",
      "Epoch 12/20\n",
      "880/870 [==============================] - 15s - loss: 1.3464 - acc: 0.5284 - val_loss: 1.5281 - val_acc: 0.4464\n",
      "Epoch 13/20\n",
      "880/870 [==============================] - 14s - loss: 1.3485 - acc: 0.5159 - val_loss: 1.4499 - val_acc: 0.4420\n",
      "Epoch 14/20\n",
      "880/870 [==============================] - 15s - loss: 1.3211 - acc: 0.5341 - val_loss: 1.3189 - val_acc: 0.5422\n",
      "Epoch 15/20\n",
      "880/870 [==============================] - 15s - loss: 1.2765 - acc: 0.5534 - val_loss: 1.3742 - val_acc: 0.5580\n",
      "Epoch 16/20\n",
      "873/870 [==============================] - 15s - loss: 1.2858 - acc: 0.5498 - val_loss: 1.1781 - val_acc: 0.5893\n",
      "Epoch 17/20\n",
      "880/870 [==============================] - 15s - loss: 1.2726 - acc: 0.5466 - val_loss: 1.1379 - val_acc: 0.6250\n",
      "Epoch 18/20\n",
      "880/870 [==============================] - 14s - loss: 1.2910 - acc: 0.5545 - val_loss: 1.1372 - val_acc: 0.6071\n",
      "Epoch 19/20\n",
      "880/870 [==============================] - 15s - loss: 1.2340 - acc: 0.5659 - val_loss: 0.9831 - val_acc: 0.6830\n",
      "Epoch 20/20\n",
      "880/870 [==============================] - 14s - loss: 1.2012 - acc: 0.5682 - val_loss: 1.1146 - val_acc: 0.6339\n",
      "Top layers training done.\n",
      "Starting with whole model training...\n",
      "Epoch 1/80\n",
      "880/870 [==============================] - 44s - loss: 1.5140 - acc: 0.4670 - val_loss: 2.6433 - val_acc: 0.4643\n",
      "Epoch 2/80\n",
      "880/870 [==============================] - 40s - loss: 1.3218 - acc: 0.5159 - val_loss: 1.8357 - val_acc: 0.5402\n",
      "Epoch 3/80\n",
      "880/870 [==============================] - 40s - loss: 1.2894 - acc: 0.5341 - val_loss: 1.1866 - val_acc: 0.6205\n",
      "Epoch 4/80\n",
      "880/870 [==============================] - 38s - loss: 1.2155 - acc: 0.5909 - val_loss: 1.3806 - val_acc: 0.5268\n",
      "Epoch 5/80\n",
      "880/870 [==============================] - 40s - loss: 1.2135 - acc: 0.5818 - val_loss: 1.0356 - val_acc: 0.6562\n",
      "Epoch 6/80\n",
      "880/870 [==============================] - 38s - loss: 1.1534 - acc: 0.5966 - val_loss: 0.9861 - val_acc: 0.6339\n",
      "Epoch 7/80\n",
      "880/870 [==============================] - 38s - loss: 1.1740 - acc: 0.5875 - val_loss: 1.0064 - val_acc: 0.6518\n",
      "Epoch 8/80\n",
      "880/870 [==============================] - 38s - loss: 1.0650 - acc: 0.6307 - val_loss: 1.0834 - val_acc: 0.6205\n",
      "Epoch 9/80\n",
      "880/870 [==============================] - 40s - loss: 1.0994 - acc: 0.6102 - val_loss: 0.9339 - val_acc: 0.7188\n",
      "Epoch 10/80\n",
      "880/870 [==============================] - 38s - loss: 1.0860 - acc: 0.6352 - val_loss: 1.0815 - val_acc: 0.6384\n",
      "Epoch 11/80\n",
      "880/870 [==============================] - 38s - loss: 1.0080 - acc: 0.6500 - val_loss: 1.0028 - val_acc: 0.6295\n",
      "Epoch 12/80\n",
      "873/870 [==============================] - 40s - loss: 1.0099 - acc: 0.6541 - val_loss: 0.6900 - val_acc: 0.7500\n",
      "Epoch 13/80\n",
      "880/870 [==============================] - 38s - loss: 0.9195 - acc: 0.6705 - val_loss: 0.7902 - val_acc: 0.7188\n",
      "Epoch 14/80\n",
      "880/870 [==============================] - 38s - loss: 0.9036 - acc: 0.6909 - val_loss: 0.7802 - val_acc: 0.7366\n",
      "Epoch 15/80\n",
      "880/870 [==============================] - 38s - loss: 0.9456 - acc: 0.6750 - val_loss: 0.9249 - val_acc: 0.7188\n",
      "Epoch 16/80\n",
      "880/870 [==============================] - 38s - loss: 0.9138 - acc: 0.6943 - val_loss: 0.8829 - val_acc: 0.6964\n",
      "Epoch 17/80\n",
      "880/870 [==============================] - 38s - loss: 0.8623 - acc: 0.7045 - val_loss: 0.9327 - val_acc: 0.6741\n",
      "Epoch 18/80\n",
      "880/870 [==============================] - 38s - loss: 0.8304 - acc: 0.7068 - val_loss: 0.8127 - val_acc: 0.7232\n",
      "Epoch 19/80\n",
      "880/870 [==============================] - 40s - loss: 0.8036 - acc: 0.7216 - val_loss: 0.8465 - val_acc: 0.6964\n",
      "Epoch 20/80\n",
      "880/870 [==============================] - 38s - loss: 0.9270 - acc: 0.6648 - val_loss: 0.7976 - val_acc: 0.7333\n",
      "Epoch 21/80\n",
      "880/870 [==============================] - 38s - loss: 0.9371 - acc: 0.6841 - val_loss: 0.8073 - val_acc: 0.7321\n",
      "Epoch 22/80\n",
      "880/870 [==============================] - 38s - loss: 0.9495 - acc: 0.6636 - val_loss: 0.8787 - val_acc: 0.6830\n",
      "Epoch 23/80\n",
      "880/870 [==============================] - 38s - loss: 0.8919 - acc: 0.6841 - val_loss: 0.8541 - val_acc: 0.7277\n",
      "Epoch 24/80\n",
      "880/870 [==============================] - 38s - loss: 0.8462 - acc: 0.6943 - val_loss: 0.8373 - val_acc: 0.7143\n",
      "Epoch 25/80\n",
      "880/870 [==============================] - 38s - loss: 0.8682 - acc: 0.7239 - val_loss: 0.9472 - val_acc: 0.6696\n",
      "Epoch 26/80\n",
      "880/870 [==============================] - 38s - loss: 0.9018 - acc: 0.6989 - val_loss: 1.0056 - val_acc: 0.6562\n",
      "Epoch 27/80\n",
      "880/870 [==============================] - 38s - loss: 0.8475 - acc: 0.7011 - val_loss: 0.8998 - val_acc: 0.7366\n",
      "Epoch 28/80\n",
      "873/870 [==============================] - 38s - loss: 0.7576 - acc: 0.7297 - val_loss: 0.7085 - val_acc: 0.7500\n",
      "Epoch 29/80\n",
      "880/870 [==============================] - 40s - loss: 0.7792 - acc: 0.7307 - val_loss: 0.8201 - val_acc: 0.7634\n",
      "Epoch 30/80\n",
      "880/870 [==============================] - 38s - loss: 0.6883 - acc: 0.7761 - val_loss: 0.7309 - val_acc: 0.7411\n",
      "Epoch 31/80\n",
      "880/870 [==============================] - 38s - loss: 0.6657 - acc: 0.7659 - val_loss: 0.7713 - val_acc: 0.7545\n",
      "Epoch 32/80\n",
      "880/870 [==============================] - 40s - loss: 0.7141 - acc: 0.7375 - val_loss: 0.6941 - val_acc: 0.7723\n",
      "Epoch 33/80\n",
      "880/870 [==============================] - 38s - loss: 0.6832 - acc: 0.7602 - val_loss: 0.7748 - val_acc: 0.7378\n",
      "Epoch 34/80\n",
      "880/870 [==============================] - 38s - loss: 0.7333 - acc: 0.7466 - val_loss: 0.8690 - val_acc: 0.7232\n",
      "Epoch 35/80\n",
      "880/870 [==============================] - 38s - loss: 0.7337 - acc: 0.7523 - val_loss: 0.9746 - val_acc: 0.6920\n",
      "Epoch 36/80\n",
      "880/870 [==============================] - 40s - loss: 0.7509 - acc: 0.7352 - val_loss: 0.7402 - val_acc: 0.7768\n",
      "Epoch 37/80\n",
      "880/870 [==============================] - 40s - loss: 0.7115 - acc: 0.7602 - val_loss: 0.5843 - val_acc: 0.8214\n",
      "Epoch 38/80\n",
      "880/870 [==============================] - 38s - loss: 0.7386 - acc: 0.7341 - val_loss: 0.8866 - val_acc: 0.7098\n",
      "Epoch 39/80\n",
      "880/870 [==============================] - 38s - loss: 0.7474 - acc: 0.7375 - val_loss: 0.7668 - val_acc: 0.7589\n",
      "Epoch 40/80\n",
      "880/870 [==============================] - 38s - loss: 0.7274 - acc: 0.7534 - val_loss: 0.9160 - val_acc: 0.7321\n",
      "Epoch 41/80\n",
      "880/870 [==============================] - 38s - loss: 0.7315 - acc: 0.7557 - val_loss: 0.6064 - val_acc: 0.8080\n",
      "Epoch 42/80\n",
      "880/870 [==============================] - 38s - loss: 0.6783 - acc: 0.7545 - val_loss: 0.7423 - val_acc: 0.7232\n",
      "Epoch 43/80\n",
      "880/870 [==============================] - 41s - loss: 0.6579 - acc: 0.7864 - val_loss: 0.7864 - val_acc: 0.7679\n",
      "Epoch 44/80\n",
      "873/870 [==============================] - 39s - loss: 0.5787 - acc: 0.8053 - val_loss: 0.9696 - val_acc: 0.6830\n",
      "Epoch 45/80\n",
      "880/870 [==============================] - 38s - loss: 0.5959 - acc: 0.8023 - val_loss: 0.7455 - val_acc: 0.7500\n",
      "Epoch 46/80\n",
      "880/870 [==============================] - 38s - loss: 0.5584 - acc: 0.8182 - val_loss: 0.6962 - val_acc: 0.7822\n",
      "Epoch 47/80\n",
      "880/870 [==============================] - 38s - loss: 0.6186 - acc: 0.7807 - val_loss: 0.7348 - val_acc: 0.7857\n",
      "Epoch 48/80\n",
      "880/870 [==============================] - 38s - loss: 0.6225 - acc: 0.7807 - val_loss: 0.6402 - val_acc: 0.7545\n",
      "Epoch 49/80\n",
      "880/870 [==============================] - 38s - loss: 0.5664 - acc: 0.8091 - val_loss: 0.7033 - val_acc: 0.7545\n",
      "Epoch 50/80\n",
      "880/870 [==============================] - 38s - loss: 0.6144 - acc: 0.7818 - val_loss: 0.6621 - val_acc: 0.7723\n",
      "Epoch 51/80\n",
      "880/870 [==============================] - 38s - loss: 0.6141 - acc: 0.7795 - val_loss: 0.8906 - val_acc: 0.7500\n",
      "Epoch 52/80\n",
      "880/870 [==============================] - 38s - loss: 0.6373 - acc: 0.7875 - val_loss: 0.7572 - val_acc: 0.7411\n",
      "Epoch 53/80\n",
      "880/870 [==============================] - 38s - loss: 0.5644 - acc: 0.7955 - val_loss: 0.8098 - val_acc: 0.7589\n",
      "Epoch 54/80\n",
      "880/870 [==============================] - 38s - loss: 0.5836 - acc: 0.8011 - val_loss: 0.8677 - val_acc: 0.7143\n",
      "Epoch 55/80\n",
      "880/870 [==============================] - 38s - loss: 0.6526 - acc: 0.7761 - val_loss: 0.9981 - val_acc: 0.6875\n",
      "Epoch 56/80\n",
      "880/870 [==============================] - 38s - loss: 0.6372 - acc: 0.7864 - val_loss: 0.7104 - val_acc: 0.7500\n",
      "Epoch 57/80\n",
      "880/870 [==============================] - 38s - loss: 0.6117 - acc: 0.7818 - val_loss: 0.6964 - val_acc: 0.7723\n",
      "Epoch 58/80\n",
      "880/870 [==============================] - 38s - loss: 0.5496 - acc: 0.8114 - val_loss: 0.7563 - val_acc: 0.7589\n",
      "Epoch 59/80\n",
      "873/870 [==============================] - 38s - loss: 0.5953 - acc: 0.7984 - val_loss: 0.7275 - val_acc: 0.7422\n",
      "Epoch 60/80\n",
      "880/870 [==============================] - 38s - loss: 0.5054 - acc: 0.8318 - val_loss: 0.8156 - val_acc: 0.7545\n",
      "Epoch 61/80\n",
      "880/870 [==============================] - 38s - loss: 0.4996 - acc: 0.8330 - val_loss: 0.7935 - val_acc: 0.7455\n",
      "Epoch 62/80\n",
      "880/870 [==============================] - 38s - loss: 0.5581 - acc: 0.8034 - val_loss: 0.9633 - val_acc: 0.7277\n",
      "Epoch 63/80\n",
      "880/870 [==============================] - 38s - loss: 0.5133 - acc: 0.8250 - val_loss: 0.6933 - val_acc: 0.7857\n",
      "Epoch 64/80\n",
      "880/870 [==============================] - 38s - loss: 0.5222 - acc: 0.8068 - val_loss: 0.7652 - val_acc: 0.7723\n",
      "Epoch 65/80\n",
      "880/870 [==============================] - 38s - loss: 0.5167 - acc: 0.8261 - val_loss: 0.8370 - val_acc: 0.7455\n",
      "Epoch 66/80\n",
      "880/870 [==============================] - 38s - loss: 0.5093 - acc: 0.8170 - val_loss: 0.7215 - val_acc: 0.7679\n",
      "Epoch 67/80\n",
      "880/870 [==============================] - 38s - loss: 0.4997 - acc: 0.8364 - val_loss: 0.7659 - val_acc: 0.7723\n",
      "Epoch 68/80\n",
      "880/870 [==============================] - 38s - loss: 0.4737 - acc: 0.8341 - val_loss: 0.9058 - val_acc: 0.7500\n",
      "Epoch 69/80\n",
      "880/870 [==============================] - 41s - loss: 0.5042 - acc: 0.8307 - val_loss: 0.7630 - val_acc: 0.7500\n",
      "Epoch 70/80\n",
      "880/870 [==============================] - 38s - loss: 0.4815 - acc: 0.8375 - val_loss: 0.8042 - val_acc: 0.7500\n",
      "Epoch 71/80\n",
      "880/870 [==============================] - 38s - loss: 0.5388 - acc: 0.8114 - val_loss: 0.6899 - val_acc: 0.7902\n",
      "Epoch 72/80\n",
      "880/870 [==============================] - 38s - loss: 0.5485 - acc: 0.8091 - val_loss: 0.7673 - val_acc: 0.7822\n",
      "Epoch 73/80\n",
      "880/870 [==============================] - 38s - loss: 0.5313 - acc: 0.8102 - val_loss: 0.8401 - val_acc: 0.6964\n",
      "Epoch 74/80\n",
      "880/870 [==============================] - 38s - loss: 0.4947 - acc: 0.8330 - val_loss: 0.7638 - val_acc: 0.7812\n",
      "Epoch 75/80\n",
      "873/870 [==============================] - 37s - loss: 0.4803 - acc: 0.8213 - val_loss: 0.7524 - val_acc: 0.7991\n",
      "Epoch 76/80\n",
      "880/870 [==============================] - 38s - loss: 0.3887 - acc: 0.8591 - val_loss: 0.7926 - val_acc: 0.7634\n",
      "Epoch 77/80\n",
      "880/870 [==============================] - 38s - loss: 0.3816 - acc: 0.8773 - val_loss: 0.8087 - val_acc: 0.7411\n",
      "Epoch 78/80\n",
      "880/870 [==============================] - 38s - loss: 0.4068 - acc: 0.8568 - val_loss: 0.7297 - val_acc: 0.7455\n",
      "Epoch 79/80\n",
      "880/870 [==============================] - 38s - loss: 0.4124 - acc: 0.8466 - val_loss: 0.9901 - val_acc: 0.7411\n",
      "Epoch 80/80\n",
      "880/870 [==============================] - 38s - loss: 0.4591 - acc: 0.8352 - val_loss: 0.8565 - val_acc: 0.7902\n",
      "Whole model training done\n"
     ]
    }
   ],
   "source": [
    "print \"Training ResNet50\"\n",
    "categorizer = Categorizer(len(CLASS_NAMES)). \\\n",
    "    build_model(ResNet50). \\\n",
    "    compile_model(). \\\n",
    "    fine_tune(\n",
    "        CLASS_NAMES.values(),\n",
    "        train_data_dir,\n",
    "        validation_data_dir,\n",
    "        batch_size=16,\n",
    "        num_only_top_epochs=20,\n",
    "        num_whole_model_epochs=80,\n",
    "        best_model_path='best_models/ResNet50.hdf5',\n",
    "        tensorboard_logs_path='tensorboard_logs/ResNet50'\n",
    "    )\n",
    "del categorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "img_path = '/Users/Josevi/product_images/photo/9760-1.jpg'\n",
    "img = load_img(img_path, target_size=(224, 224))\n",
    "x = img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "imgplot = plt.imshow(mpimg.imread(img_path))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%time\n",
    "preds = model.predict(x)\n",
    "print preds\n",
    "#print('Predicted:', decode_predictions(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def show_sample(X, y, prediction=-1):\n",
    "    im = X\n",
    "    print y\n",
    "    #y = np.flip(y, axis=0)\n",
    "    y_label = class_name[np.nonzero(y)[0][0]]\n",
    "    plt.imshow(im)\n",
    "    if prediction >= 0:\n",
    "        plt.title(\"Class = %s, Predict = %s\" % (y_label, class_name[prediction]))\n",
    "    else:\n",
    "        plt.title(\"Class = %s\" % (y_label))\n",
    "\n",
    "    plt.axis('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for X_batch, Y_batch in train_generator:\n",
    "    for i in range(len(Y_batch)):\n",
    "        show_sample(X_batch[i, :, :, :], Y_batch[i])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
